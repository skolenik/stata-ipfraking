\inserttype[st0001]{article}
\author{S. Kolenikov}{%
  Stanislav Kolenikov\\Abt SRBI\\kolenikovs@srbi.com
}
\title[Raking survey data]{Calibrating survey data using iterative proportional fitting (raking)}
\maketitle

\begin{abstract}
This article introduces package \stcmd{ipfraking} implementing weight calibration
procedures known as iterative proportional fitting,
or raking, of complex survey weights.
The package is capable of handling a large number of control
variables and trimming the weights in various ways.
It also provides diagnostic tools for the weights it creates.
Examples of its usage are given,
and the suggested workflow is discussed.

\keywords{\inserttag, survey, calibration, weights, raking}
\end{abstract}

\section{Introduction and background}

Large scale social, behavioral and health data are often collected
via complex survey designs that may involve some or all of stratification,
multiple stages of selection and unequal probabilities of selection
\citep{korn:graubard:1995,korn:graubard:1999}.
In an ideal setting, varying probabilities of selection are
accounted for by using the Horvitz-Thompson estimator of the totals
\citep{horvitz:thompson:1952,thompson:1997}, and the remaining
sampling fluctuations can be further ironed out by
post-stratification \citep{holt:smith:1979}.
However, on top of the planned differences in probabilities of obtaining
a response from a sampled unit, non-response is a practical problem
that has been growing more acute over the recent years
\citep{groves:dillman:eltinge:little:2001,pew:2012}.
The analysis weights that are provided along with the public use
microdata by data collecting agencies are designed to account
for unequal probabilities of selection, non-response, and other factors 
affecting imbalance between the population and the sample, thus making 
the analyses conducted on such micordata generalizable to the target population.
In this paper, I shall discuss the specific issue in the process of creating
survey weights: 
calibrating survey weights to known control totals to ensure
that the resulting weighted data are representative of the population
of interest.

\subsection{Population totals}
\label{subsec:totals}

For a given finite population $\mathcal U$ of units indexed $i=1,\ldots,N$,
the interests of survey statisticians often lie in estimating the
population total of a variable $Y$
\begin{equation}
   T[Y] = \sum_{i \in \mathcal{U}} Y_i
   \label{eq:total:pop}
\end{equation}
(As is customary in sampling texts, the population quantities will be denoted
with capital letters, and the sample quantities, with lowercase letters.
The finite population is denoted as $\mathcal{U}$, and the sample drawn from it,
as $\mathcal{S}$. The indices of units in the population are $i\in \mathcal{U}$, and
those of units in the sample, $j \in \mathcal{S}$.)
A lot of other analytical problems can be cast in terms of estimating
the totals of the existing or auxiliary variables, and then expressing
the quantities of substantive interest (means, ratios, regression
coefficients) as functions of these totals \citep{skinner:1989}.
For instance, the population mean,
such as mean income or
the average number of hours per week spent watching TV,
is the ratio of the totals
$$
    \bar Y = \frac{T[Y]}{T[1]}
$$
where the denominator is a somewhat unusual total of a variable
identically equal to 1, i.e., the estimator of the population size
(if the latter is unknown). The mean for a domain $\mathcal{D}$
(mean income of females; TV hours of teenagers) is also a ratio of totals
$$
    \bar Y_\mathcal{D} = \frac{T[YZ]}{T[Z]}
$$
where $Z_i=1$ when the unit is in the domain, and 0 otherwise.
Estimation of totals is thus the cornerstone building block of survey
statistics.

\subsection{Probability weights}

Suppose now that a sample $\mathcal S$ of $n$ units indexed by $j=1,\ldots,n$
is taken from $\mathcal U$. If the probability to select the
$i$-th unit is known to be $\pi_i$, then
the {\it probability weights}, or {\it design weights}, are given by
the inverse probability of selection:
\begin{equation}
   w_{1i} = \pi_i^{-1}
   \label{eq:prob:weight}
\end{equation}
With these weights, an unbiased
(design-based, non-parametric) estimator
of the total (\ref{eq:total:pop}) is \citep{horvitz:thompson:1952}
\begin{equation}
   t_{1}[y] = \sum_{j \in \mathcal{S}} \frac{y_j}{\pi_j}
   \equiv \sum_{j \in \mathcal{S}} w_{1j} y_j
   \label{eq:total:sample},
\end{equation}
The subindex $1$ indicates that the weights $w_{1i}$ were
used in obtaining this estimator. Probability weights protect
the end user from potentially informative sampling designs, in which
the probabilities of selection are correlated with outcomes, and 
the design-based methods generally ensure that inference can be generalized
to the finite population even when the statistical models used
by analysts and researchers are not specified correctly
\citep{pfeff:1993,binder:roberts:2003}.

\subsection{Calibrated weights}
\label{subsec:calibration}

Often, survey statisticians have auxiliary information on the units
in the frame. Some of that information can be used at the sampling
stage to inform stratification and clustering. When creating areal
probability samples of human populations, the survey designers have
geographic information on the sampled units: from strata
that can be defined as regions, states or provinces, and the primary
sampling units that can be defined as districts, counties or census tracts;
down to the address of the sampled household. When creating establishment
survey samples, the frame information that survey statisticians may have
at their disposal can include the industry classification code(s) and size
of establishment (number of employees or the total revenue).
When drawing samples from lists of persons, such as members of a
professional organization or patients of a hospital, the frame information
can include age and gender of an individual. If such additional information
is available, it is usually beneficial to include it at the sampling stage
to create more efficient designs. Unequal probabilities of selection
are then controlled with probability weights, implemented
as \stcmd{[pw=}{\it exp}\stcmd{]} in Stata (and can be permanently
affixed to the data set with \stcmd{svyset} command).

In many situations, however, usable information is not available beforehand,
and may only appear in the collected data. In the above example with
areal samples, the census totals of the age and gender
distribution of the population may exist, but age and gender of 
the sampled units is unknown until the survey measurement is taken on them.
It is still possible to capitalize on this additional data by
adjusting the weights in such a way that the reweighted data
conforms to these known figures. The procedures to perform these
reweighting steps are generally known as {\it weight calibration}
\citep{deville:sarndal:1992,deville:sarndal:sautory:1993,%
kott:2006,kott:2009,sarndal:2007}.

Suppose there are several (categorical) variables, referred to
as {\it control variables}, that are available for both
the population and the sample
(age groups, race, gender, educational attainment, etc.).
The {\it post-stratification} adjustment consists of breaking down
the population into post-stratification cells defined
by specific levels of the control variables (i.e., a cell in a multivariate
contingency table), and adjusting the weights within each cell
so that the weights sum to the known total.
Thus for the unit $j$ in the (population-level) post-stratification
cells $\mathcal{C}_k$, the post-stratified weight is
\begin{equation}
   w_{2j} = w_{1j} \frac%
        {\sum_{i \in \mathcal{U}} \One[i \in \mathcal{C}_k]}%
        {\sum_{l \in \mathcal{S}} w_{1l} \One[l \in \mathcal{C}_k]}%
   \label{eq:ps:weight}
\end{equation}
where $\One[\cdot]$ is an indicator function taking the value of one
when its argument is true, and zero otherwise.
While the probability weight $w_{1i}$ is fixed, the post-stratified weight
$w_{2j}$ is random, as it depends on the random sample sizes of
$\mathcal{C}_k \cap \mathcal{S}$.
The existing Stata \stcmd{svyset} options, \stcmd{poststrata()} and \stcmd{postweight()}
(see \svyref{poststratification}), handle this situation and,
in particular, provide appropriate standard errors. The total estimator
based on $w_{2j}$ will be naturally denoted as $t_2[y]$, and the expression
for it coincides with (\ref{eq:total:sample}) by replacing the probability
weights $w_{1j}$ with post-stratified weights $w_{2j}$.

In more complex situations, using say five calibration variables,
such as gender, age groups, race/ethnicity, education, and urbanicity,
leads to five-way contingency tables that will likely have zero or
very small count cells. \citet{batt:izra:hoag:fran:2009} suggest
collapsing the categories that contain less than 5\% of either
the sample cases or the population units. For a sample of size
$n=1,000$ typical for social science studies or public opinion polls,
this recommendation translates to cells of size
$\sum_{j \in \mathcal{S}} \One[i \in \mathcal{C}_k] \le 50$.
Instead of adjusting every cell of a multi-way table, weight calibration 
can target adjusting only the margins, or low level interactions,
via an iterative optimization aimed at satisfying
the {\it control totals} for the control variables $\mathbf{x}=(x_1, \ldots, x_p)$:
\begin{equation}
    \sum_{j \in \mathcal{S}} w_{3j} \mathbf{x}_j
    = T [ \mathbf{X}_j  ]
    \label{eq:control:totals}
\end{equation}
where the right hand side is assumed to be known from a census or
a higher quality survey.
\citet{deville:sarndal:1992} framed the problem of finding a suitable
set of weights as that of constrained optimization with the control
equations (\ref{eq:control:totals}) serving as constraints,
and optimization targeted at making the discrepancy between
the design weights $w_{1j}$ and calibrated weights
$w_{3j}$ as close as possible, in a suitable sense. Again,
the appropriate total estimator can be denoted as $t_3[y]$.

\subsection{Raking algorithm}
\label{subsec:raking:algorithm}

An early algorithm to perform weight calibration is often attributed to
\citet{deming:stephan:1940} who used it to adjust the counts in a contingency
table to satisfy the known margins in log-linear analysis.
In applications to the survey weights, the algorithm is described below.
At a basic level, this algorithm consists of an outer cycle that checks
convergence criteria, and an inner cycle that iterates over the control
variables. The multi-index notation of the intermediate weights,
$w_j^{k,v}$, indicates the weight of unit $j$ computed in the outer cycle $k$
after post-stratifying with respect the $v$-th variable. Thus $k$ runs from $1$
to a predefined maximum number of iterations $K$, and $v$ runs from zero
(indicating the input weight to a given iteration) through $1$ (indicating
adjustment with respect to the first control variable) to $p$ (indicating
adjustment with respect to the last control variable).

{\it Algorithm 1: basic raking}
\begin{enumerate}
    \item Initialize the iteration counter $k\leftarrow 0$
          and the weights as $w_j^{0,p} \leftarrow w_{1j}$.
          (That is, use the base weights to initialize the raked weight;
          the superscript $0,p$ is only used for consistency with notation
          used in the next step.)
    \item Increment the iteration counter $k \leftarrow k+1$,
          update the weights $w_j^{k,0} \leftarrow w_j^{k-1,p}$.
          (That is, use the end result of the previous outer cycle iteration
          to initialize the weights for the current outer cycle iteration.)
    \item Inner cycle: go over the control variables $v=1,\ldots,p$ and update the weights
          $$
            w_j^{k,v} =
                \left\{
                \begin{array}{ll}
                    w_j^{k,v-1} \dfrac{ T[X_v] }{ \sum_{l \in \mathcal{S}} w_l^{k,v-1} x_{vl} },
                        & x_{vj} \neq 0 \\
                    w_j^{k,v-1},
                        & x_{vj} = 0
                \end{array}
                \right.
          $$
          (That is, post-stratify with respect to the $v$-th control variable.)
    \item If discrepancies between the weighted totals $\sum_{j \in \mathcal{S}} w_j^{k,p} x_v$
          and the target totals $T[X_v]$ are within prespecified tolerances for all
          $v=1,\ldots,p$, declare convergence and exit to step \ref{alg1:exit}.
    \item If the number of iterations $k$ reaches a prespecified limit $K$,
          declare non-convergence, issue corresponding warnings, and exit to step \ref{alg1:exit}.
    \item Otherwise, return to step 2. (That is, the achieved accuracy of the control targets
          insufficient, and more work is needed.)
    \item Return the weights $w_j^{k,p}$ at the final stage as the calibrated weights.
            \label{alg1:exit}
\end{enumerate}


In practice, control totals are usually expressed as population counts
or proportions in categories of discrete variables (such as gender,
race/ethnicity or education level groups).
The control variables are then 0/1 indicators representing the particular groups
or their low-level interactions.
Effectively, the algorithm implements post-stratification adjustment
(\ref{eq:ps:weight}) treating each control variable as the post-stratification
variable, and cycling over these variables within each iteration.
In terms of multivariate optimization,
this algorithm proceeds by optimizing over a each margin in sequence.
While it is very simple and very explicit in terms
of the algebra involved, it is also much slower compared
to Newton-Raphson-based methods.

\citet{deming:stephan:1940} stated that the algorithm minimizes the quadratic
discrepancy
$$
    \sum_{j \in \mathcal{S}} \frac{ (w_{1j}-w_{3j})^2}{w_{1j}}
$$
under the calibration constraints (\ref{eq:control:totals}).
However, the quadratic problem can be solved explicitly to
produce linear calibrated weights which lead to estimates
identical to the generalized regression (GREG) estimates
\citep[Case 1]{deville:sarndal:1992}. The raking algorithm instead solves
the optimization problem with objective function that can be expressed as
\citep[Case 2]{deville:sarndal:1992}
\begin{equation}
    \sum_{j \in \mathcal{S}} w_{3j} \ln( w_{3j}/w_{1j}) - w_{3j} + w_{1j}
    \label{eq:raking:discrepancy}
\end{equation}

\subsection{Variance estimation}
\label{subsec:variance}

Besides the primary challenge of finding a good set of weights
(which is generally solved through iterative optimization),
an additional methodological challenge with calibrated estimators
is variance estimation. If variables $x_1,\ldots,x_p$ were used
for weight calibration, then the asymptotic variance of the calibrated
estimator of the survey variable $y$ is
\begin{equation}
    \mathbb{V}\bigl\{t_{m}[y]\bigr\} = \sum_{k,l \in \mathcal{U}}
        (\pi_{kl}-\pi_k \pi_l)
        \frac{Y_k - \mathbf{X}_k ' \mathbf{B}}{\pi_k}
        \frac{Y_l - \mathbf{X}_l ' \mathbf{B}}{\pi_l}, m=2,3,
    \label{eq:var:calibrated}
\end{equation}
where $\mathbf{B}$ is the vector of coefficients from the
census regression,
\begin{equation}
    \mathbf{B}
    = \bigl( \sum_{i \in \mathcal{U}} \mathbf{X}_i \mathbf{X}_i' )^{-1}
      \sum_{i \in \mathcal{U}} \mathbf{X}_i Y_i
    \label{eq:census:regression},
\end{equation}
This variance can be estimated with
\begin{equation}
    v\bigl\{t_{m}[y]\bigr\} = \sum_{k,l \in \mathcal{S}}
        \frac{\pi_{kl}-\pi_k \pi_l}{\pi_{kl}}
        \frac{y_k - \mathbf{x}_k ' \mathbf{b}}{\pi_k}
        \frac{y_l - \mathbf{x}_l ' \mathbf{b}}{\pi_l}, m=2,3,
    \label{eq:varest:calibrated}
\end{equation}
where the regression coefficients now solve the sample
regression problem:
\begin{equation}
    \mathbf{b}
    = \bigl( \sum_{j \in \mathcal{S}} w_j \mathbf{x}_j \mathbf{x}_j' )^{-1}
      \sum_{j \in \mathcal{S}} w_j \mathbf{x}_j y_j
    \label{eq:sample:regression}
\end{equation}
In regression (\ref{eq:sample:regression}),
either the probability weights $w_{1j}$ or the calibrated weights
$w_{2j}, w_{3j}$ can be used. The estimator (\ref{eq:varest:calibrated})
is difficult to use in practice, especially with the publicly released
versions of the data. First, this estimator utilizes the original design weights
$w_{1i}=\pi_i^{-1}$. Hence, the publicly released data set must include
both the calibrated weights and the design weights, which may create
confusion. Second, the end user
of the data must be given the set of the control variables, which may
not be possible if confidential variables were used in calibration. Third,
this estimator is not necessarily implemented in survey packages
(a third party package \stcmd{calibest} implements (\ref{eq:varest:calibrated})
in Stata). Finally, the estimator requires second order selection probabilities,
which are rarely computed in practice. The latter is a very general issue
with the Horvitz-Thompson estimator (\ref{eq:total:sample}), as well.
Its variance is
\begin{equation}
    \mathbb{V}\bigl\{t_{1}[y]\bigr\} = \sum_{k,l \in \mathcal{U}}
        (\pi_{kl}-\pi_k \pi_l)
        \frac{Y_k}{\pi_k}
        \frac{Y_l}{\pi_l}
    \label{eq:var:ht}
\end{equation}
While the second order selection probabilities are nominally required
for this estimator, in practice simplifications are taken, e.g.,
to approximate the actual design as the stratified two-stage sample
in which the primary sampling units are drawn with replacement
(as is done, for example, in \stcmd{nhanes2} \svyref{} manual example
dataset). Due to these complications, variance estimation with
calibrated data usually proceeds along the lines of
replicate variance estimation methods \citep{shao:1996,kolenikov:2010}.

When the control totals are obtained from another survey,
the sampling variability of the latter should be taken
into account \citep{dever:valliant:2010}. For instance,
to calibrate population surveys conducted in the USA,
the American Community Survey \citep{acs:2009}
is often used for demographic variables,
and the National Health Interview Survey \citep{nhis:2000} for phone usage.
These very large scale surveys have sample sizes in the hundreds of thousands.
For typical surveys with sample sizes in hundreds to low thousands,
the impact on the standard errors is in the second or the third
decimal point, and is usually ignored.


\subsection{Pros and cons of weight calibration}
\label{subsec:pro:con}

By comparing expressions (\ref{eq:var:calibrated}) and
(\ref{eq:var:ht}), we can identify the source of efficiency gains
associated with weight calibration. If the survey variable
$y$ is associated with calibration variables $x_1,\ldots,x_p$,
in the sense of having a non-trivial $R^2$ in the census regression
(\ref{eq:census:regression}),
then the calibrated estimator is (asymptotically)
more efficient than the direct Horvitz-Thompson estimator
by a factor of $1-R^2$.

Weight calibration can also reduce non-response
and coverage errors
\citep{chang:kott:2008,kott:2006,lund:sarn:1999}, which
feature prominently as some of the most important issues
that survey community currently faces \citep{groves:2006}.
However, for weight calibration to be successful in
reducing the non-response bias, the control variables
need to correlated with the response propensity and/or the outcome
variables \citep{bethlehem:2002,judkins:etal:2007}.

Weight calibration comes with some costs, too. From an analytic perspective,
manipulating the weights almost inevitably leads to increase in their
variation, which in turn leads to increases in the design effects.
For the unequal probability sample without stratification or clustering,
\citet{korn:graubard:1999} show that the design effect is
\begin{equation}
    {\rm DEFF}_w = \frac{\sum_{j \in \mathcal{S}} w_j^2 }%
       {\Bigl(\sum_{j \in \mathcal{S}} w_j\Bigr)^2}
    = 1 + {\rm CV}_w^2
    \label{eq:deff:weights}
\end{equation}
where ${\rm CV}_w$ is the coefficient of variation of the weights
(a simple standard deviation divided by the simple mean).
In practice, I have encountered increases of this coefficient
of variation between 20\% and 100\% on the relative scale,
or between 0.2 and 1.5 on the absolute scale, for design effects
varying between 1 and 2 in the typical public opinion surveys.
From a practical perspective, weight calibration requires
additional time expenses by statisticians preparing the data,
which increases the cost of the survey and the time elapsing
between the end of the data collection period and delivery
of the final data set. Additionally, as noted in section
\ref{subsec:variance}, variance estimation with calibrated
data tends to get complicated.

\subsection{Weight trimming}
\label{subsec:trimming}

As expression (\ref{eq:deff:weights}) shows, it is undesirable
for a survey to have a large spread of weights \citep{theberge:2000}.
Otherwise, many survey
estimates are unduly affected by the observations with large weights,
while those with small weights make but minimal contributions.
The impact of the observations with high weights will be exacerbated
in the analysis of domains, where these observations will stand apart
even more given the smaller sizes of domains.
For these reasons, weights are often {\it trimmed}: the largest
weights are reduced (say, all the weights greater than the largest allowable
number are reduced to that number), and the smallest weights are increased,
so that for all $j$, $L \le w_{3j} \le U$ for some absolute limits $L$ and $U$.
Alternatively, the relative change in weights can be constrained:
for all $j$, $l \le w_{3j}/w_{1j} \le u$ for some ratio limits $l$ and $u$.
Weight trimming may introduce bias, so the amount of trimming needs to be
seen as a trade-off between an apparent efficiency improvement
and latent bias \citep{elliott:2008}.

With trimming, the modified algorithm implemented in
\stcmd{ipfraking} proceeds as follows.
(See Section \ref{subsec:syntax} for the syntax diagram, and in particular
Section \ref{subsubsec:trimming} for specification of
the trimming options. If not specified otherwise, the default values
are $U=u=+\infty$, $L=l=0$.)


{\it Algorithm 2: raking with simultaneous trimming}

\begin{enumerate}
    \item Initialize the outer cycle iteration counter $k\leftarrow 0$.
          Initialize the weights $w_j^{0,p} \leftarrow w_{1j}$.
          Set $D_0 = \infty$ (This is a notation introduced for consistency 
          of notation in step  \ref{step:check:weight:conv}.)
    \item Increment the outer cycle iteration counter $k \leftarrow k+1$,
          update the weights $w_j^{k,0} \leftarrow w_j^{k-1,p}$.
          \label{step:next:cycle}
    \item Initialize the inner cycle over control variables: $v \leftarrow 1$
    \item Update the weights using the $v$-th variable
        as the post-stratification variable:
        $$
            w_j^{k,v} =
                \left\{
                \begin{array}{ll}
                    w_j^{k,v-1} \dfrac{ T[X_v] }{ \sum_{l \in \mathcal{S}} w_l^{k,v-1} x_{vl} },
                        & x_{vj} \neq 0 \\
                    w_j^{k,v-1},
                        & x_{vj} = 0
                \end{array}
                \right.
          $$
          \label{step:update:weights}
    \item If \stcmd{trimfrequency} option is specified as \stcmd{often},
        perform weight trimming:
        $$
            w_j^{k,v} \leftarrow \min\bigl( w_j^{k,v}, U, u w_j^{0,p} \bigr),
        $$
        $$
            w_j^{k,v} \leftarrow \max\bigl( w_j^{k,v}, L, l w_j^{0,p} \bigr)
        $$
        That is, trim the weights that are greater than $U$ in absolute terms and/or
        have increased by more than a factor of $u$ from the initial weight; reduce such weights
        to the largest allowed value. Likewise, trim the weights that are less than $L$
        in absolute terms and/or have dropped by more than a factor of $l$ from the initial
        weight; increase such weights to the smallest allowed value.
        \label{step:trimfreq:often}
    \item Increment the internal cycle counter $v \leftarrow v+1$
    \item If $v \le p$, cycle back to step \ref{step:update:weights}.
        Otherwise the inner cycle over the control variables is completed; proceed to the next step.
    \item If \stcmd{trimfrequency} option is specified as \stcmd{sometimes},
        perform weight trimming:
        $$
            w_j^{k,p} \leftarrow \min\bigl( w_j^{k,p}, U, u w_j^{0,p} \bigr),
        $$
        $$
            w_j^{k,p} \leftarrow \max\bigl( w_j^{k,p}, L, l w_j^{0,p} \bigr)
        $$
        \label{step:trimfreq:sometimes}
    \item If the largest change in weights
        $$
            D_k = \max_{j \in \mathcal{S}}
                \Bigl| \frac{w_j^{k,p}}{w_j^{k-1,p}} - 1 \Bigr|
        $$
        is less than or equal to tolerance $\delta_D$ (given in \stcmd{tolerance} option),
        declare convergence of weights and go to step \ref{step:trimfreq:once}. 
        If $D_k > D_{k-1}$, $k>1$, the algorithm
        may be diverging; stop the outer cycle iterations, issue a non-convergence
        message and go to step \ref{step:trimfreq:once}. Otherwise
        (i.e., if $\delta_D < D_k < D_{k-1}$), move to the next step:
        there is some room for weight improvement.
        \label{step:check:weight:conv}
    \item If the number of the outer cycle iterations $k$ reaches a prespecified limit $K$,
          stop the outer cycle iterations and issue a non-convergence message. 
          Otherwise, cycle back to step \ref{step:next:cycle}.
    \item If \stcmd{trimfrequency} option is specified as \stcmd{once},
        perform weight trimming:
        $$
            w_j^{k,p} \leftarrow \min\bigl( w_j^{k,p}, U, u w_j^{0,p} \bigr),
        $$
        $$
            w_j^{k,p} \leftarrow \max\bigl( w_j^{k,p}, L, l w_j^{0,p} \bigr)
        $$
        \label{step:trimfreq:once}
    \item If discrepancies between the weighted totals
        $\sum_{j \in \mathcal{S}} w_j^{k,p} x_v$
        and the target totals $T[X_v]$ are greater than
        prespecified tolerances $\delta_T$ (\stcmd{ctrltolerance} option),
        $$
            \Bigl| \frac{ T[X_v] - \sum_{j \in \mathcal{S}} w_j^{k,p} x_v }%
                        {T[X_v]+1} \Bigr| > \delta_T
        $$
        for at least one $v=1,\ldots,p$,
        issue a warning message (see Section \ref{subsec:tbshooting}).
        \label{step:check:ctotal:conv}
    \item Return the weights $w_j^{k,p}$ as calibrated weights and exit.
\end{enumerate}

The algorithm may be exited for three possible reasons: reaching
the maximum number of iterations (indicative of lack of convergence),
finding that the changes in weights started diverging,
or by reaching the state where the weights do not change from one iteration
of post-stratification and possibly trimming to the next. Even in the latter
case, convergence of the weights does not imply convergence of the weighted
totals to their targets. Hence, there are qualitatively three possible
outcomes of running \stcmd{ipfraking}:
\begin{enumerate}
    \item The weights have converged as checked in step
         \ref{step:check:weight:conv}, and the weighted control totals
         are within tolerances from their targets, as checked in
         step \ref{step:check:ctotal:conv}. The raked weights
         are most likely safe to use, although additional quality control checks,
         including computation of the DEFF (\ref{eq:deff:weights}),
         histograms and tabulations with the main variables of interest 
         would be recommended.
    \item The weights have converged as checked in step
         \ref{step:check:weight:conv}, but the weighted control totals
         are not sufficiently close to their targets, as checked in
         step \ref{step:check:ctotal:conv}. The raked weights
         should be reviewed, and may not be safe to use.
         This often happens when the trimming options are
         too aggressive, when the data and the control totals
         are incompatible, or when the control totals themselves
         are poor (e.g., the matrices sum to different values,
         of which \stcmd{ipfraking} will issue an error message).
    \item The weights have not converged after the pre-specified number
        of iterations, or started diverging. Again, the resulting weights
        are likely to be unsatisfactory. The number of iterations should
        be increased, the tolerances should be decreased, or
        \stcmd{nodivergence} option can be specified if optimization aborted
        because the weight convergence criteria went up.
\end{enumerate}

\subsection{Other weight calibration programs}

There has been a number of packages with similar functionality
that are circulating in Stata community. Nick Winter's \stcmd{survwgt}
\citep{winter:2002}
is the most robust and versatile of these, and its \stcmd{survwgt rake}
subcommand implements the same raking algorithm as the basic algorithm
of \stcmd{ipfraking}. The functionality of \stcmd{survwgt} also includes
valuable capabilities to create the balanced repeated replication (BRR) 
and jackknife replicate weights
\citep{kolenikov:2010}, as well as non-response cell adjustments.
One feature that \stcmd{survwgt} does not have is trimming.

A more recent raking package is \stcmd{ipfweight} \citep{bergmann:2011}.
%% It is VERY memory inefficient!!! And not particularly robust in variable names
It implements the basic raking, and provides relative trimming similar
to \stcmd{trimfrequency(often)}.

Another user-contributed Stata package, \stcmd{maxentropy}
\citep{wittenberg:2010}, implements Case 4 of \citet{deville:sarndal:1992} using
Newton-Raphson optimization with analytical second derivatives,
and is much faster than \stcmd{ipfraking} described here.

Compared to these packages, \stcmd{ipfraking} was developed to work in
the weight production environment of a survey company. To be an effective tool,
the weight calibration procedure should not only produce the correct figures,
but also provide extensive diagnostics and robustness checks that can potentially
be analyzed later in semi-automated fashion, and be robust and fail softly with
incorrectly specified inputs. For instance, all of the above packages rely on the user
to match the variables and their targets, and some are relatively fragile numerically
when the initial weights generate totals that are far off their targets.
In \stcmd{ipfraking}, as you will shortly see, the match between variables 
and their targets is implemented internally through metadata
(variable names and values) stored in Stata target matrices, as the necessary
variables and their categories are picked up by \stcmd{ipfraking} from the targets.
Thus the number of necessary inputs, and hence the likelihood of the user error
(through incorrect ordering of variables and their categories) is reduced.
The targets, in turn, can be easily obtained from the calibration data sets
such as American Community Survey. Also, \stcmd{ipfraking} defines convergence in
terms of values of weights rather than the target discrepancies as done in other
packages. It thus allows the possibility of the raking procedure converging
in computational sense (weights stop changing from one iteration to the next),
and then diagnoses the statistical convergence, i.e., whether the targets
are being satisfied.

Besides the internal convergence diagnostics, the weights produced by \stcmd{ipfraking}
were compared to those produced by \stcmd{survwgt} and \stcmd{ipfweight}
as a certification step \citep{gould:2001}, and were found to be identical
within numerical accuracy.

\section{Package description}

\subsection{Syntax}
\label{subsec:syntax}

\begin{stsyntax}
ipfraking
\optif\
\optin\
\optweight\
,
\underbar{ctot}al({\it matname} [{\it matname \ldots}])
\optional{
\underbar{gen}erate(\newvarname)
replace
double
\underbar{iter}ate(\num)
\underbar{tol}erance(\num)
\underbar{ctrltol}erance(\num)
trace
\underbar{nodiv}ergence
trimhiabs(\num)
trimhirel(\num)
trimloabs(\num)
trimlorel(\num)
trimfrequency(once|sometimes|often)
double
meta
nograph
}
\end{stsyntax}

\hangpara
Note that the weight statement \stcmd{[pw=\varname]} is required, and must contain the initial weights.

\subsection{Options}

\subsubsection{Required options}

\hangpara
\stcmd{\underbar{ctot}al(}{\it matname} \LB{\it matname \ldots}\RB\stcmd{)}
supplies the names of the matrices that contain the control
totals, as well as meta-data about the variables to be used
in calibration.

\begin{sttech}
The row and column names of the control total matrices
(see \pref{matrix rownames}) should be formatted as follows.
\begin{itemize}
    \item \stcmd{rownames}: the name of the control variable
    \item \stcmd{colnames}: the values the control variables takes
    \item \stcmd{coleq}: the name of the variable for which total is computed;
          typically it is identically equal to 1.
\end{itemize}
See examples in Section \ref{sec:examples}.
\end{sttech}

\hangpara
\stcmd{\underbar{gen}erate(\newvarname)}
contains the name of the new variable to contain the raked weights.

\hangpara
\stcmd{replace} indicates that the weight variable supplied in the
\stcmd{[pw=\varname]} expression should be overwritten with the new weights.

One and only one of \stcmd{generate()} or \stcmd{replace} must be specified.

\subsubsection{Options to control convergence}

\begin{sttech}
Convergence in \stcmd{ipfraking} is defined in terms of the maximum relative
change in weights:
\begin{equation}
    D_k = \max_{j \in \mathcal{S}} \frac{|w_j^{k,p}-w_j^{k-1,p}|}{w_j^{k-1,p}}
    \label{eq:conv:ratio:weights}
\end{equation}
When $D_k$ is small, $D_k < \delta_D$, it means that the weights stop
changing between iterations, i.e., the algorithm came to its steady state.
On the other hand, if $D_k > D_{k-1}$, it means that the algorithm
may start diverging, at which point it might be reasonable to terminate it.
See step \ref{step:check:weight:conv} of Algorithm 2 in Section \ref{subsec:trimming}.

Once the algorithm terminates, it also checks whether the control totals
are satisfied. Specifically, for each of the control total matrices
$M_1, \ldots, M_p$, the relative difference vs.\ the corresponding weighted
sample totals $\hat M_1, \ldots, \hat M_p$ is computed:
\begin{equation}
    m_c = {\rm mreldif}( \hat M_c, M_c )
    \label{eq:conv:mreldif}
\end{equation}
where the maximum relative difference of two matrices
$$
{\rm mreldif}(A,B) = \max_{ij} \frac{|a_{ij}-b_{ij}|}{1+|b_{ij}|}
$$
as defined in \dref{functions}. A control relation is satisfied
if $m_c < \delta_T$; otherwise, a warning is issued.
See step \ref{step:check:ctotal:conv} of Algorithm 2 in Section \ref{subsec:trimming}.

Iterations continue until either $k = K$, a specified number of iterations; 
$D_k < \delta_D$; or $D_k > D_{k-1}$.
\end{sttech}

\hangpara
\stcmd{\underbar{tol}erance(\num)} defines the $D_k$-convergence
criterion, i.e., $\delta_D$. The default is $\delta_D = 10^{-6}$.

\hangpara
\stcmd{\underbar{iter}ate(\num)} specifies the maximum number
of iterations $K$. The default is $K=2000$.

\hangpara
\stcmd{\underbar{nodiv}ergence} overrides the check
that $D_k > D_{k-1}$, i.e., ignores this termination condition.

\hangpara
\stcmd{\underbar{ctrltol}erance(\num)} defines the criterion $\delta_T$ to
assess the accuracy of the control totals. It does not impact
iterations or convergence criteria; it only serves as the final quality control check
after the algorithm terminates as defined above. The default value is $\delta_T=10^{-6}$.

\hangpara
\stcmd{trace} requests a trace plot to be added. See
Section \ref{subsec:example:trace}.

\subsubsection{Trimming options}
\label{subsubsec:trimming}

\hangpara
\stcmd{trimhiabs(\num)} specifies the upper bound $U$ on the greatest
    value of the raked weights.  The weights that
    exceed this value will be trimmed down, so that
    $w_{3j} \le U$ for every $j\in\mathcal{S}$.

\hangpara
\stcmd{trimhirel(\num)} specifies the upper bound $u$ on the adjustment
    factor over the baseline weight. The weights
    that exceed the baseline times this value will be trimmed down,
    so that $w_{3j} \le u w_{1j}$ for every $j\in\mathcal{S}$.

\hangpara
\stcmd{trimloabs(\num)} specifies the lower bound $L$ on the smallest value
    of the raked weights.  The weights that are smaller than this value will
    be increased, so that $w_{3j} \ge L$ for every $j\in\mathcal{S}$.

\hangpara
\stcmd{trimlorel(\num)} specifies the lower bound $l$ on the adjustment factor
    over the baseline weight.  The weights that are smaller than the baseline
    times this value will be increased, so that
    $w_{3j} \ge l w_{1j}$ for every $j\in\mathcal{S}$.

\hangpara
\stcmd{trimfreqency({\it keyword})} specifies when the trimming operations
    are to be performed. The following keywords are recognized:

\morehang \stcmd{often} means that trimming will be performed
    after each marginal adjustment, i.e., within each iteration of the inner
    cycle inside Step \ref{step:trimfreq:often} of Algorithm 2.

\morehang \stcmd{sometimes} means that trimming will be performed
    after a full set of variables has been used for post-stratification, i.e.,
    at the end of each outer cycle iteration at step \ref{step:trimfreq:sometimes} of
    Algorithm 2. This is the default behavior if any of the numeric trimming
    options above are specified.

\morehang \stcmd{once}
    means that trimming will be performed after the outer loop converges
    at step \ref{step:trimfreq:once} of Algorithm 2.

The numeric trimming options \stcmd{trimhiabs(\num)}, \stcmd{trimhirel(\num)}, 
\stcmd{trimloabs(\num)}, \stcmd{trimlorel(\num)} can be specified in any combination,
or entirely omitted to produce untrimmed weights. By default, there is no trimming.
See Section \ref{subsec:example:trimming} for examples.

\subsubsection{Miscellaneous options}

\hangpara
\stcmd{double} specifies that the new variable named in \stcmd{generate()}
option should be generated as double type. See \dref{data types}.

\hangpara
\stcmd{meta} puts the name(s) of the control vectors and the achieved control
    accuracies $m_c$ as characteristics stored with the variable specified in
    \stcmd{generate()} option. See Section \ref{subsec:example:meta}.

\hangpara
\stcmd{nograph} omits the histogram of the calibrated weights, which can be
used to speed up \stcmd{ipfraking} once the diagnostics on the weights 
are completed (e.g., in replicate weight production).

\subsection{Utility programs}

Besides the main weight calibration program, \stcmd{ipfraking} package provides
two additional utility programs to create and manipulate
\stcmd{ipfraking}-compatible matrices.

\begin{stsyntax}
mat2do
{\it matrix\_name}
using
{\it do\_file\_name}
,
\optional{
replace
append
list
type
}
\end{stsyntax}

\stcmd{mat2do} stores the values and the attributes (row and column names)
of a Stata matrix as a do-file. By running this do-file, the matrix can be fully
reproduced. The names of the matrix and the do-file are required.

\hangpara
\stcmd{replace} overwrites the existing file.

\hangpara
\stcmd{append} adds the code the the existing do-file.

\hangpara
\stcmd{list} adds \stcmd{matrix list} command to the end of the do-file,
so that when the {\it do\_file\_name} is executed, the listing is provided
for verification.

\hangpara
\stcmd{type} lists the matrix and the resulting do-file.




\begin{stsyntax}
xls2row
{\it matrix\_name}
using
{\it filename}
,
cellrange([{\it start}]:[{\it end}])
sheet({\it name})
over(\varname)
\optional{
scale(\num)
}
\end{stsyntax}

The utility program \stcmd{xls2row} reads the calibration totals from
the specified Excel file and stores them in the matrix {\it matrix\_name}.
The name of the Excel file, the range of cells and the name of
the sheet to take the values from are required, and specified in the same
way as in \stcmd{import excel} (see \dref{import excel}).
Mathematically speaking, \stcmd{xls2row} performs a
vec-transformation of the matrix read from an Excel sheet, i.e., stores the result by columns.
The \stcmd{matrix coleq} of the resulting
    matrix is the convention name \stcmd{\_one}.

To optimize the performance, note that \stcmd{xls2row} relies on
\stcmd{preserve} as an intermediate step. It is thus advisable to run
\stcmd{xls2row} upfront before loading potentially large data sets that
would otherwise be written to disk and restored back a number of times.


\hangpara
    \stcmd{cellrange()} specifies the range of cells in an Excel file, e.g., B2:D15.

\hangpara
    \stcmd{sheet()} specifies the name of the sheet in Excel file to take values from.

\hangpara
    \stcmd{over(\varname)} is the variable corresponding to the control total being
    imported from Excel. The columns of the resulting row vector {\it matrix\_name} will the
    labeled with the values of \varname (i.e., as the \stcmd{matrix colname} 
    of the matrix {\it matrix\_name}), and \varname\ itself will appear
    as the \stcmd{matrix rowname} of the matrix {\it matrix\_name}. If the number
    of categories of \varname\ does not match the number of non-missing imported
    values, an error message will be issued, and the target matrix will not be created.

\hangpara
    \stcmd{scale(\num)} optionally scales the entries of
    the resulting row vector so that they sum to the specified value.





\section{Examples}
\label{sec:examples}

\subsection{Basic syntax and input requirements}
\label{subsec:basic}

In this very simple example, I shall demonstrate the basic mechanics of
\stcmd{ipfraking}, its input requirements and output. 
These examples are intended to only demonstrate the syntax 
and the output of \stcmd{ipfraking}, and may or may not provide
substantively meaningful results.


\begin{stexample}[Example 1]

We shall work with the standard example of \stcmd{svy} data,
an excerpt from the NHANES II data set available from Stata Corp.\ website.
We shall introduce some small changes to the data so that
\stcmd{ipfraking} will have some work to do.

\begin{stlog}
\input{ipfr.example1.prep.log.tex}\nullskip
\end{stlog}

Let us now look at the matrices that will serve as an input to the
raking procedure.

\begin{stlog}
\input{ipfr.example1.list.log.tex}\nullskip
\end{stlog}

These input matrices are organized as follows. Input matrices always
have a single row, just as estimation results \stcmd{e(b)} do. The column
names follow the naming conventions of \stcmd{e(b)}, namely,
the name of the variable for which the total is being computed
(here, \stcmd{\_one}) and the numeric categories of the variable that
was used in the \stcmd{over} option (here, \stcmd{sex}, with values 1 for males
and 2 for females; and \stcmd{race}, with values 1 for whites, 2 for blacks,
and 3 for other). These values must be in an increasing order.
Since that variable is not stored in the e(b) per se,
it needs to be added to this matrix, which is done in the form of the row name.
The entries of the matrix are the totals that the weights in the categories
of the control variables need to sum up to. In this example, they are scaled
to be the population totals. Alternatively, these can be made to sum up to the
sample size, as is done sometimes in public opinion research, or to 1, which
is what \stcmd{proportion} estimation command would produce.

The input requirements in terms of control totals are thus made as simple as possible.
If a higher quality survey is available, all the survey statistician needs to do
is to obtain the totals for the categories of the control variables
using \stcmd{svy: total $\cdots$, over( $\cdots$, nolabel ) }
and save the name of that variable along with the matrix.
Note that the \stcmd{total}
is computed with \stcmd{over( \ldots, nolabel)} suboption to suppress
the otherwise informative labeling of the categories;
\stcmd{ipfraking} expects the numeric values of the categories
as column names (see \pref{matrix rownames}).
The name of the matrix itself is immaterial, but it is
a good programming practice
to have informative names \citep{mcconnell:2004}. Thus the names
of the matrices in the examples generally follow the convention
{\it data{\_}source}{\_}{\it variable}.

We are now ready to run \stcmd{ipfraking} and see what it produces.

\begin{stlog}
\input{ipfr.example1.run.log.tex}\nullskip
\end{stlog}

In this simple case with just two control variables
and the control totals that are not very different from the
existing sample totals, the procedure converged very quickly
in three iterations. A diagnostic message was produced upfront
by \stcmd{ipfraking} informing about apparent differences in
total population counts as obtained from the different
control total matrices. As a result, the control totals
for the variable that was adjusted first (\stcmd{sex})
could not match the required control totals even after the
weights converged in the sense of differing little between
iterations. Both of these warnings are only produced when
problems are encountered.

The summary table is always produced, and shows some relevant
characteristics of the original weights $w_{1j}$, the raked weights
$w_{3j}$, and the raking ratios $w_{3j}/w_{1j}$. As expected,
the coefficient of variation went up from 0.645 to 0.672.

The graphic output produced by \stcmd{ipfraking} is shown on
Figure \ref{fig:example1}. Generally, we would want to inspect these
graphs to see if there any unexpected patterns, such as highly outlying values,
gaps in the distribution (here, there are only six distinct values of the 
adjustment factor corresponding to the $2\times3$ combinations of the control
variables) or concentration near the limits of the
weight range (as is typical for trimmed weights, see below in section
\ref{subsec:example:trimming}). Also, these graphs
may inform later trimming decisions: the trimming limits can be
chosen to conform to the breaks in the distributions of
the untrimmed raked weights.

\begin{figure}[h!]
\begin{center}
\epsfig{file=ipfraking_example1}
\end{center}
\caption{Histograms of the raked weights and calibration ratios, Example 1.}
\label{fig:example1}
\end{figure}

\end{stexample}

\subsection{Preparing control matrices from scratch}
\label{subsec:acs}

In many situations, the control totals will be obtained
from outside of Stata, and need to be prepared to work
with \stcmd{ipfraking}.

\begin{stexample}[Example 2]

Suppose I wanted to calibrate
the NHANES II data set to the latest control totals available
from the US Census Bureau website. Using the tables
S0101 from the 2011 American Community Survey 1-year estimates
and NST-EST2011 from the US Census Bureau population projections,
the latest available at the time of writing this paper,
the figures displayed in Table \ref{tab:example2} can be obtained.

\begin{table}
\caption{Control totals for the 2011 US population.\label{tab:example2}}

\centering

\begin{tabular}{p{5cm}l}
    Group & Population \\
    \hline
    \multicolumn{2}{c}{~~ACS 2011 1-year estimates, Table S0101~~} \\
    Male, total & 153,267,860 \\
    Ages 20--39 & 27.4\% \\
    Ages 40--59 & 27.5\% \\
    Ages 60+    & 17.3\% \\
    Female, total & 158,324,057 \\
    Ages 20--39 & 26.0\% \\
    Ages 40--59 & 27.6\% \\
    Ages 60+    & 20.7\% \\
    \multicolumn{2}{c}{~~US Census Bureau 2011 projections, Table NST-EST2011-01~~} \\
    Northeast & 55,521,598 \\
    Midwest   & 67,158,835 \\
    South     & 116,046,736 \\
    West      & 72,864,748 \\
    \multicolumn{2}{c}{~~US Census Bureau 2011 projections, Table NC-EST2011-03~~} \\
    White     & 243,470,497 \\
    Black     & 40,750,746 \\
    Other     & 27,370,674 \\
    \hline
    Total     & 311,591,917
\end{tabular}
\end{table}

Thus, we have information in the two-way age by sex table, as well
as two additional margins. We shall need an additional sex-by-age group variable,
and we shall try to make its values somewhat informative
(e.g., the value \stcmd{12} of the variable \stcmd{sex\_age} means
the first group of sex and the second group of age):

\begin{stlog}
\input{ipfr.example2.prep.log.tex}\nullskip
\end{stlog}

With that, the matrices will have to be defined explicitly,
and their labels need to be hand-coded, too (see \pref{matrix rownames}).
Note that the US Census Bureau 2011
projections relate to the total population, while the target population
of the study is the population age 20+. Assuming that the age structure
is the same across regions and races, the control totals for region and race
need to be rescaled to the adult population to avoid the warning messages.
(More accurate figures can be obtained from ACS microdata which can be downloaded
from the U.S.\ Census Bureau website.)

\begin{stlog}
\input{ipfr.example2.mat.log.tex}\nullskip
\end{stlog}

Let us check the matrix entries and labels once again before
producing the weights.
Note that the values of the control variable categories are given in an increasing order.

\begin{stlog}
\input{ipfr.example2.list.log.tex}\nullskip
\end{stlog}

As the labels appear to be in place, let us run \stcmd{ipfraking}:

\begin{stlog}
\input{ipfr.example2.run.log.tex}\nullskip
\end{stlog}

The diagnostic plots for these weights are given in Figure \ref{fig:example2}.
They do appear to have some outlying cases (which are not very clearly seen
on these plots as they are single count observations with outlying weights),
and we shall address them in the next section with trimming.

\begin{figure}[h!]
\begin{center}
\epsfig{file=ipfraking_example2}
\end{center}
\caption{Histograms of the raked weights and calibration ratios, Example 2.}
\label{fig:example2}
\end{figure}

\end{stexample}

\subsection{Trimming options}
\label{subsec:example:trimming}


As discussed in Section \ref{subsec:trimming} above, if variability of the weights
becomes excessive, the weights can be trimmed by restricting the extremes.
Using \stcmd{ipfraking} options, upper and/or lower limits can be defined
for either the absolute values of the weights or the relative changes from
the base weights. The frequency of the trimming operations can also be controlled.
Trimming can be applied once to the final data (\stcmd{trimfreq(once)})
at step \ref{step:trimfreq:once} of Algorithm 2.
Alternatively, trimming can be applied after every full cycle over variables
at step \ref{step:trimfreq:sometimes} of Algorithm 2.
Finally, trimming can be applied after each sub-iteration
at step \ref{step:trimfreq:often} of the algorithm.

\begin{stexample}[Example 3]

Inspecting the histograms on Figure \ref{fig:example2}, it appears reasonable
to restrict the upper tail of the raked weights. A more detailed investigation
of the histogram reveals a somewhat greater concentration of the raked weights
around the value of 160,000, and sparse bars beyond 200,000. This latter number
will be used as the top cut-off point for trimming, and is provided as an input
to \stcmd{ipfraking} via option \stcmd{trimhiabs}. Also, I specified the absolute
lower bound of 2,000, which is the minimum of the original weights, but,
as the output in the previous example suggested, the calibrated weights tend to run
above 4,000, so specifying the lower limit as \stcmd{trimloabs(2000)} may not really
affect the calibration procedure.

\begin{stlog}
\input{ipfr.example3.trimabs.log.tex}\nullskip
\end{stlog}

The resulting coefficient of variation of weights, 0.857, is slightly
better than that with unrestricted range of weights, 0.872. The summary also shows
that the weights were capped at 200,000, as requested.

Setting the absolute limits on the range of the raked weights is often
very subjective. A somewhat better plan might be to set limits in terms
of the range of the adjustment factors, as shown in the next example. The relative
change in the weights can be bounded with \stcmd{trimlorel()} and \stcmd{trimhirel()} 
options.
I also demonstrate here how to use the results of \stcmd{summarize} to feed
into \stcmd{ipfraking}. While ensuring that accurate numbers are being carried
over in the context of the code, the approach is fragile for interactive 
work: simply running the single line with the sole
\stcmd{ipfraking} command that refers to the \stcmd{r()} return values
may break down if \stcmd{summarize} was not the
immediately preceding command.

\begin{stlog}
\input{ipfr.example3.trimsum.log.tex}\nullskip
\end{stlog}

\end{stexample}

Setting the trimming options too aggressively may lead to adverse
consequences. First, it may bias the estimates, as discussed in Section
\ref{subsec:pro:con}.
Second, as this example demonstrates, it can impede (statistical) convergence:
the output contains multiple warnings about targets not being achieved
within desired accuracy, while no problems were encountered without trimming.

\subsection{Tracking convergence}
\label{subsec:example:trace}

Let us now look in more detail into the issue of trimming frequency,
and demonstrate another diagnostic plot that can be produced by
\stcmd{ipfraking}.

\begin{stexample}[Example 4]

We return to the first set of options of Example 3, and
re-run the raking procedure.

\begin{stlog}
\input{ipfr.example4.sometimes.log.tex}\nullskip
\end{stlog}

The option \stcmd{trace} requests that trace plots be added to
the diagnostic plots, as shown on Figure \ref{fig:example4:sometimes}.
The trace plots are presented on the absolute scale and on the log scale.
The exponentially declining discrepancy appears to be a general phenomenon.
In other words, after the first few iterations, 
discrepancy between the currently weighted totals to the control totals roughly follows
the rate of $\rm{const} \times \alpha^k$ for some $\alpha<1$, where $k$ is
the (outer cycle) iteration number. When convergence is very slow or the sample
size is very large, this rule may be helpful in determining the number
of iterations necessary to achieve the required accuracy, and hence
the expected computing time. Zero cross-cells and collinearity between 
the control variables may make the convergence factor $\alpha$ close to 1 thus 
hampering convergence. This happens when the control variables have 
very similar meaning, such as age and grade of children: it is impossible
to have children of age 8 in grade 10.
Also, sets of interactions of categorical variables, such as interactions 
of age group and education along with age group and race, are guaranteed to 
produce zero cells in the cross-tabulation: it is impossible to have 
any observations in the cells defined say by 
(age under 40 interacted with higher education) on one margin against
(age above 60 interacted with white race) on the other.

\begin{figure}[!th]
\begin{center}
\epsfig{file=ipfraking_example4_sometimes}
\end{center}
\caption{Diagnostic plots for Example 4.}
\label{fig:example4:sometimes}
\end{figure}

While \stcmd{trimfreq(sometimes)} is the default in presence
of other trimming options, the behavior can be changed
with explicit specification of trimming frequency. Note that slightly
different weights will be produced that way.

\begin{stlog}
\input{ipfr.example4.often.log.tex}\nullskip
\end{stlog}

In this example, trimming the weights after adjusting each of the margins
led to fewer iterations. This may or may not translate to lower overall
computing times as more computing is performed within each iteration.

\end{stexample}

\subsection{Metadata}
\label{subsec:example:meta}

The results of raking operations can be stored with the newly created
weight variables for later review and reproduction of the results.
Let us reproduce the example in the previous section adding all the metadata
available:

\begin{stexample}[Example 5]

\begin{stlog}
\input{ipfr.example5.log.tex}\nullskip
\end{stlog}

\end{stexample}

The following characteristics are stored with the newly created weight variable
(see \pref{char}).

\begin{tabular}{ll}
    \stcmd{command} & The full command as typed by the user \\
    {\it matrix name} & The relative matrix difference from the corresponding \\
                    & control total, see \dref{functions} \\
    \stcmd{trimhiabs}, \stcmd{trimloabs}, & Corresponding trimming options,
                    if specified \\
    \stcmd{trimhirel}, \stcmd{trimlorel}, & \\
    \stcmd{trimfrequency} & \\
    \stcmd{maxctrl} & the greatest \stcmd{mreldif} between the targets \\
                    & and the achieved weighted totals \\
    \stcmd{objfcn}  & the value of the relative weight change $D_k$ (\ref{eq:conv:ratio:weights}) 
                    at exit \\
    \stcmd{converged} & whether \stcmd{ipfraking} exited due to convergence (1) \\
                    & vs. due to an increase in the objective function \\
                    & or reaching the limit on the number of iterations (0)
\end{tabular}

Also, \stcmd{ipfraking} stores the notes regarding the control matrices
used, and which of the margins did not match the control totals, if any.
See \dref{notes}.

\subsection{Replicate weights}

As discussed in Section \ref{subsec:variance}, one of the greater challenges
of weight calibration is ensuring that variance estimates take into account
the greater precision achieved by adjusting the sample towards the fixed
population quantities. As estimating the variances using linearization
is cumbersome, replicate variance estimation may be more attractive.

\begin{stexample}[Example 6]

The simplest code for calibrated replicate weights is obtained by calling
\stcmd{ipfraking} from within \stcmd{bsweights} \citep{kolenikov:2010}
which can pass the name of a replicate weight variable to an arbitrary
calibration routine. In this example, we shall use the same settings
as in Section \ref{subsec:acs} and thus we shall have the calibrated weight
\stcmd{rakedwgt2} which was produced in that example as the main weight
for which the bootstrap weights provide the measure of sampling variability.

\begin{stlog}
\input{ipfr.example6.bsw.log.tex}\nullskip
\end{stlog}

The options of \stcmd{bsweights} request 310 replicate weights
(a multiple of 31 strata), resample one less PSU than available in
a given stratum, and obtain the first-order balance within a stratum.
With the 2 PSU/stratum design and these options, \stcmd{bsweights}
produces random half-samples of data. The at-character \stcmd{@} is a placeholder
for the name of the replicate weight variable.
For explanations of these and other options of \stcmd{bsweights},
see \citet{kolenikov:2010}. The procedure took about 3 minutes
on a laptop computer, which can be considered moderately
computationally intensive beyond interactive.
A new option of \stcmd{ipfraking} in the above code is
\stcmd{nograph} that suppresses the histograms.
The additional asserts \citep{gould:2003:tip3} following the bootstrap
weight generation demonstrate how the minimal quality assurance
can be done on the bootstrap weights in the weight production workflow.


A more compact set of weights can be developed based on the existing
BRR weights and a slightly more explicit code cycling over the weight
variables:

\begin{stlog}
\input{ipfr.example6.brr.log.tex}\nullskip
\end{stlog}

The data can be analyzed with the standard \stcmd{svy} prefix,
and the standard errors will appropriately capture the efficiency
gains from weight calibration. No additional action is required
for the analyst or researcher.

\end{stexample}

{\bf CAUTION:} the input weights for the replicate weight calibration
must be the probability replicate weights. The existing NHANES II weights
have been adjusted for non-response and calibrated by the data provider,
and are used above for demonstration purposes only.

\section{Error messages and troubleshooting}
\label{subsec:tbshooting}

\subsection{Critical errors}

The following critical errors will stop execution of
\stcmd{ipfraking}.

\noindent
{\tt pweight is required}

\morehang
    The \stcmd{[pweight=\ldots]} component of \stcmd{ipfraking}
    syntax is required. Probability weights must be specified as
    inputs to \stcmd{ipfraking}.

\noindent
    {\tt ctotal() is required}

    \morehang
    The \stcmd{ctotal()} component of \stcmd{ipfraking}
    syntax is required. Names of the matrices containing the
    control totals must be specified.

    \noindent
    {\tt one and only one of generate() or replace must be specified}

    \morehang
    Either \stcmd{generate()} option with the name of the new variable
    must be supplied to \stcmd{ipfraking}, or \stcmd{replace} to replace
    the variable specified in \stcmd{[pw=\ldots]} statement.

    \noindent
    {\tt raking procedure appears diverging}

    \morehang
    The maximum relative difference of weights $D_k$ has increased from
    the previous
    iteration. This may or may not indicate a problem. Re-run \stcmd{ipfraking}
    with \stcmd{nodivergence} option to override the warning.

    \noindent
    {\tt cannot process matrix {\it matrix{\_}name}}

    \morehang
    For whatever reason, \stcmd{ipfraking} could not process this matrix.
    The matrix may not have been defined or the variables in this matrix
    cannot be found.

    \noindent
    {\tt variable {\it varname} corresponding to the control matrix
    {\it matrix{\_}name} \\ not found}

    \morehang
    The variables contained in row or column names of this matrix
    cannot be found.

    \noindent
    {\tt {\it varname1} and {\it varname2} variables are not compatible}

    \morehang
    When running \stcmd{total} {\it varname1}\stcmd{, over(}{\it varname2}\stcmd{)},
    an error was encountered. One of the variables may be a string variable
    or have missing values resulting in an empty estimation sample.

    \noindent
    {\tt categories of {\it varname} do not match in the control {\it matrix{\_}name} \\
    and in the data (nolab option)}

    \morehang
    There was a mismatch in the categories of {\it varname} found in the data
    and in the control matrix {\it matrix{\_}name}. This could happen for any of the
    following reasons: (i) there were more categories in one than in the other;
    (ii) the entries are in the wrong order in the control matrix; (iii) the labels
    in the control matrix do not correspond to the category values in the data set;
    (iv) the control matrix was obtained via \stcmd{total}
    {\it varname2}\stcmd{, over(}{\it varname}\stcmd{)}, but \stcmd{nolabel} suboption
    of \stcmd{over()} was omitted, and the labels of the control matrix may include
    some unexpected text. Tabulate {\it varname} without labels, and compare the results
    to the matrix listing of the {\it matrix{\_}name}.

    \noindent
    {\tt cannot compute controls for {\it matrix{\_}name} over
    {\it varname} with the current \\ weights}

    \morehang
    This is a generic error message that something bad happened while
    \stcmd{ipfraking} was computing the totals for the current set of weights.
    This error message should generally be very rare, but as computing
    the totals may be the slowest operation of the iterative optimization
    process, stopping \stcmd{ipfraking} with a {\it Ctrl+Break} combination or
    the {\it Break} GUI button may produce this error message.

    \noindent
    {\tt trimhiabs|trimloabs|trimhirel|trimlorel must be a positive number}

    \morehang
    One or more of the trimming options are given as a non-positive number
    or a non-number.

    \noindent
    {\tt trimhiabs must be greater than trimloabs}

    \noindent
    {\tt trimhirel must be greater than trimlorel}

    \morehang
    The trimming parameters are illogical (the lower bound is greater than the upper bound).
    Respecify the values of the trimming parameters.

\bigskip

\subsection{Other errors and warnings}

The following warning messages may be produced by
\stcmd{ipfraking}. The program will continue running, but you must
double-check the results for potential problems.

\noindent
    {\tt the totals of the control matrices are different}

    \morehang
    The sum of values of the control matrices are different.
    These sums will be listed for review. Convergence is still
    possible, but some of the control total checks are likely to fail.

    \noindent
    {\tt trimfrequency() option is specified without numeric settings; will be \\ ignored}

    \morehang
    The option \stcmd{trimfrequency()} was specified without any numeric trimming options.
    There is no way to interpret this, and \stcmd{ipfraking} will proceed without
    trimming.

    \noindent
    {\tt trimfrequency() option is specified incorrectly, assume default value \\ (sometimes)}

    \morehang
    Something other than \stcmd{often}, \stcmd{sometimes} or \stcmd{once} was supplied
    in \stcmd{trimfrequency}, and the default value is being used instead.

    \noindent
    {\tt raking procedure did not converge}

    \morehang
    The maximum number of iterations was reached, but weights never met the convergence
    criteria (see step \ref{step:check:weight:conv} of Algorithm 2 in Section \ref{subsec:trimming}). 
    The user may want to increase the number of iterations or relax convergence criteria.

    \noindent
    {\tt the controls {\it matrix{\_}name} did not match}

    \morehang
    After convergence of weights was declared, \stcmd{ipfraking}
    checked again the control totals, and found that the results
    differed from the target for one or more of the control total
    matrices. Any of the following can cause this: (i) the sum of
    entries of this particular matrix differs from the others;
    (ii) the trimming options are too restrictive, and do not allow
    the weights to adjust enough; (iii) the problem may not have a
    solution due to incompatible control totals or a bad sample.

    \noindent
    {\tt division by zero weighted total encountered with
    {\it matrix{\_}name} control}

    \morehang
    The weights for a category of the control variable summed 
    to zero. \stcmd{ipfraking} will skip calibration over this
    variable and proceed to the next one.

    \noindent
    {\tt \# missing values of {\it varname} encountered; convergence will be impaired}

    \morehang
    A control variable has missing values in the calibration sample.
    There is little way for \stcmd{ipfraking} to figure out how to deal
    with the weights for the observations with missing values. The user would need 
    either to restrict the sample to non-missing values of all control variables,
    to impute the missing values or to create a separate category for the missing
    values of a given control variable (which may lead to difficulties in defining
    valid population control totals for it).

\section*{Acknowledgements}

The author is grateful to Ben Phillips, Andrew Burkey and Brady West,
as well as the editor and an anonymous referee,
who suggested additional functionality and provided helpful comments
to improve the readability of this article. The opinions stated in this paper
are of the author only, and do not represent the position of Abt SRBI.

\bibliographystyle{sj}
% \bibliography{everything}
\bibliography{ipfraking}

\appendix

\section*{Appendix: Common notation}

\begin{tabular}{llp{9.5cm}}
    $\mathcal{C}_k$ & Sec. \ref{subsec:calibration} & Calibration cell \\
    $D_k$ & (\ref{eq:conv:ratio:weights}) & Maximum relative difference of weights 
            from iteration $k-1$ to iteration $k$ \\
    $\delta_D$ & Sec. \ref{subsec:trimming} & Convergence criteria for $D_k$ \\
    $\delta_T$ & Sec. \ref{subsec:trimming} & Quality control criteria for control totals \\
    $i$ & Sec. \ref{subsec:totals} & Subscript $i$ usually applies to units in population \\
    $j$ & Sec. \ref{subsec:totals} & Subscript $j$ usually applies to units in sample \\
    $k$ & Sec. \ref{subsec:trimming} & The outer cycle iteration number \\
    $K$ & Sec. \ref{subsec:trimming} & The maximum number of the outer cycle iterations \\
    $l$ & Sec. \ref{subsec:trimming} & Relative limit on weights: all the weights will be made
        $\ge (l \times$ the input weight) \\
    $l$ & & summation index, where I run out of other traditional integer letters \\
    $L$ & Sec. \ref{subsec:trimming} & Absolute limit on weights: all the weights will be made 
        $\ge L$ \\
    $n$ & & Sample size; number of sampled units \\
    $N$ & & Population size; number of units in population or frame \\
    $\pi_i$ & & Probability of selection of unit $i$
            specified by the sampling design \\
%
    $\mathcal{S}$ & Sec. \ref{subsec:totals} & Sample; set of sampled units \\
    $T[y]$ & (\ref{eq:total:pop}) & Population-based total of variable $[y]$ \\
    $t_m[y]$ & (\ref{eq:total:sample}) & Sample-based weighted estimate of the total
        $T[y]$; subscript $m=1,2,3$ indicates the type of weights used in computing
        the total \\
    $u$ & Sec. \ref{subsec:trimming} & Relative upper limit on weights: all the weights will be made
        $\le (u \times$ the input weight) \\
    $L$ & Sec. \ref{subsec:trimming} & Absolute upper limit on weights: all the weights will be made
        $\le U$ \\
    $\mathcal{U}$ & Sec. \ref{subsec:totals} & Universe or population; set of units in population \\
%
    $w_{1i}$ & (\ref{eq:prob:weight}) & Probability (design) weights;
        inverse probability of selection \\
    $w_{2j}$ & (\ref{eq:ps:weight}) & Post-stratified weights;
        random; depend on group sizes in sample; analytically computable \\
    $w_{3j}$ & Sec. \ref{subsec:calibration} & Calibrated (raked) weights; random;
        require iterative optimization \\
%
    $x_v$ & Sec. \ref{subsec:raking:algorithm} & the $v$-th calibration (control)
        variable, $v=1,\ldots,p$. The population total $T[x_v]$ is known. \\
\end{tabular}

\begin{aboutauthor}
  Stanislav (Stas) Kolenikov is a Senior Survey Statistician at Abt SRBI.
  His research interests include
  applications of statistical methods in public opinion research,
  such as advanced sampling techniques, survey weighting,
  calibration, missing data imputation, and variance estimation.
  Besides survey statistics, Stas has extensive experience developing and applying
  statistical methods in social sciences, with focus on structural equation
  modeling and microeconometrics. He has been writing Stata programs since
  1998 when Stata was version 5.
\end{aboutauthor}
