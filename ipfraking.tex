\inserttype[st0001]{article}
\author{S. Kolenikov}{%
  Stanislav Kolenikov\\Abt SRBI\\kolenikovs@srbi.com
}
\title[Raking survey data]{Calibrating survey data using iterative proportional fitting (raking)}
\maketitle

\begin{abstract}
This article introduces a package implementing weight calibration
procedures known as iterative proportional fitting,
or raking, of complex survey weights.
The package is capable of handling a large number of control
variables and trimming the weights in various ways.
It also provides diagnostic tools for the weights it creates.
Examples of its usage are given,
and the suggested workflow is discussed.

\keywords{\inserttag, survey, calibration, weights, raking}
\end{abstract}

\section{Introduction and background}

Large scale social, behavioral and health data are usually collected
via complex survey designs that involve some or all of stratification,
multiple stages of selection and unequal probabilities of selection
\citep{korn:graubard:1995,korn:graubard:1999}.
In an ideal setting, varying probabilities of selection can be fully
accounted for by using the Horvitz-Thompson estimator of the totals
\citep{horvitz:thompson:1952,thompson:1997}, and the remaining
sampling fluctuations can be further ironed out by
post-stratification \citep{holt:smith:1979}.
However, on top of the planned differences in probabilities of obtaining
a response from a sampled unit, non-response is a practical problem
that has been growing more acute over the recent years
\citep{groves:dillman:eltinge:little:2001,pew:2012}.
In this paper, I shall discuss the specific issue of creating
survey weights, and, even more specifically, concentrate on
calibrating survey weights to the known control totals, to ensure
that the resulting weighted data are representative of the population
of interest.

For a given finite population $\mathcal U$ of units indexed $i=1,\ldots,N$,
the interests of survey statisticians often lie in estimating the
population total of a variable $y$
\begin{equation}
   T[y] = \sum_{i \in \mathcal{U}} y_i
   \label{eq:total:pop}
\end{equation}
A lot of other analytical problems can be cast in terms of estimating
the totals of the existing or auxiliary variables, and then expressing
the quantities of the substantive interest (means, ratios, regression
coefficients) as functions of these totals \citep{skinner:1989}. In that
sense, estimation of totals is the cornerstone building block of survey
statistics.

\subsection{Probability weights}

Suppose now that a sample $\mathcal S$ of $n$ units indexed by $j=1,\ldots,n$
is taken from $\mathcal U$. If the probability to select the
$i$-th unit is known to be $\pi_i$, then an unbiased
(design-based, non-parametric) estimator
of the total (\ref{eq:total:pop}) is \citep{horvitz:thompson:1952}
\begin{equation}
   t_{w1}[y] = \sum_{j \in \mathcal{S}} \frac{y_j}{\pi_j}
   \equiv \sum_{j \in \mathcal{S}} w_{1j} y_j
   \label{eq:total:sample},
\end{equation}
where the probability weights, or design weights, are given by
the inverses of probability of selection:
\begin{equation}
   w_{1i} = \pi_i^{-1}
   \label{eq:prob:weight}
\end{equation}
The subindex $w1$ stands to indicate that the weights $w_{1i}$ were
used in obtaining this estimator. Probability weights protect
the end user from potentially informative sample designs, and generally
design-based methods ensure that inference can be generalized
to the finite population even when the statistical models used
by analysts and researchers are not specified correctly
\citep{pfeff:1993,binder:roberts:2003}.

\subsection{Calibrated weights}

Often, survey statisticians have auxiliary information on the units
in the frame. Some of that information can be used at the sampling
stage to inform stratification and clustering. When creating areal
probability samples of human populations, the survey designers have
geographic information on the sampled units: from strata
that can be defined as regions, states, or provinces, and the primary
sampling units that can be defined as districts, counties, or census tracts;
down to the address of the sampled household. When creating establishment
survey samples, the frame information that survey statisticians may have
at their disposal can include the industry classification code(s) and size
of establishment (number of employees or the total revenue).
When drawing samples from lists of persons, such as members of a
professional organization or patients of a hospital, the frame information
can include age and gender of an individual. If such additional information
is available, it is usually beneficial to include it at the sampling stage
to create more efficient designs. Unequal probabilities of selection
are then controlled with probability weights, implemented
as \stcmd{[pw=}{\it exp}\stcmd{]} in Stata (and can be permanently
affixed to the data set with \stcmd{svyset} command).

In many situations, however, usable information is not available beforehand,
and may only appear in the collected data. In the above example with
areal samples, there may exist census figures on the age and gender
distribution of the population, but age and gender of the sampled units
is unknown until the survey measurement is taken on them.
It is still possible to capitalize on this additional data by
adjusting the weights in such a way that the reweighted data
conforms to these known figures. The procedures to perform these
reweighting steps are generally known as {\it weight calibration}
\citep{deville:sarndal:1992,deville:sarndal:sautory:1993,kott:2006,kott:2009,sarndal:2007}.

Suppose there are several (categorical) variables, referred to
as {\it control variables}, that are available for both
the population and the sample
(groups of age, gender, race, educational attainment, etc.).
The {\it post-stratification} adjustment consists of breaking down
the population into post-stratification cells defined
by specific levels of the control variables (i.e., a cell in a multivariate
contingency table), and adjusting the weights within each cell.
Thus for the unit $l$ in the (population-level) post-stratification cells $\mathcal{C}_k$, the post-stratified weight is
\begin{equation}
   w_{2l} = w_{1l} \frac%
        {\sum_{i \in \mathcal{U}} \One[i \in \mathcal{C}_k]}%
        {\sum_{j \in \mathcal{S}} w_{1j} \One[i \in \mathcal{C}_k]}%
   \label{eq:ps:weight}
\end{equation}
where $\One[\cdot]$ is an indicator function taking the value of 1
when its argument is true, and zero otherwise.
While the probability weight $w_{1i}$ is fixed, the post-stratified weight
$w_{2j}$ is random, as it depends on the random sample sizes of
$\mathcal{C}_k \cap \mathcal{S}$.
The existing Stata tools, \stcmd{poststrata()} and \stcmd{postweight()}
options (see \svyref{poststratification}) handle the situation and,
in particular, provide appropriate standard errors.

In more complex situations, using say five calibration variables,
such as gender, age groups, race/ethnicity, education, and urbanicity,
leads to five-way contingency tables that will likely have zero or
very small count cells. \citet{batt:izra:hoag:fran:2009} suggest
collapsing the categories that contain less than 5\% of either
the sample cases or the population units. For a sample of size
$n=1,000$, this translates to cells of the size
$\sum_{j \in \mathcal{S}} \One[i \in \mathcal{C}_k] \le 50$.
Weight calibration has to proceed
via an iterative optimization that is aimed at satisfying
the {\it control totals} for the control variables $x_1, \ldots, x_p$:
\begin{equation}
    \sum_{j \in \mathcal{S}} w_{3j} \mathbf{x}_j
    = T [ \mathbf{x}_j  ]
    \label{eq:control:totals}
\end{equation}
where the right hand side is assumed to be known from a census or
a higher quality survey.
\citet{deville:sarndal:1992} framed the problem of finding a suitable
set of weights as that of constrained optimization with the control
equations (\ref{eq:control:totals}) serving as constraints,
and optimization targeted at making the discrepancy between
the design weights $w_{1j}$ and calibrated weights
$w_{3j}$ as close as possible, in a suitable sense.

\subsection{Raking algorithm}
\label{subsec:raking:algorithm}

An early algorithm to perform weight calibration is often attributed to
\citet{deming:stephan:1940} who used it to adjust the counts in a contingency
table to satisfy the known margins in log-linear analysis.
In applications to the survey weights, the algorithm is as follows.
\begin{enumerate}
    \item Initialize the iteration counter $k\leftarrow 0$
          and the weights as $w_j^{0,p} \leftarrow w_{1j}$.
    \item Increment the iteration counter $k \leftarrow k+1$,
          update the weights $w_j^{k,0} \leftarrow w_j^{k-1,p}$.
    \item Cycle over the control variables $l=1,\ldots,p$: update the weights
          $$
            w_j^{k,l} =
                \left\{
                \begin{array}{ll}
                    w_j^{k,l-1} \dfrac{ T[x_l] }{ \sum_{j \in \mathcal{S}} w_j^{k,l} x_l },
                        & x_{lj} \neq 0 \\
                    w_j^{k,l-1},
                        & x_{lj} = 0
                \end{array}
                \right.
          $$
    \item If discrepancies between the weighted totals $\sum_{j \in \mathcal{S}} w_j^{k,p} x_l$
          and the target totals $T[x_l]$ are within a prespecified tolerances for all
          $l=1,\ldots,p$, declare convergence and exit.
    \item If the number of iterations $k$ reaches a prespecified limit $K$,
          declare non-convergence and exit.
    \item Otherwise, return to step 2.
\end{enumerate}
In practice, control totals are usually expressed as population counts in
categories of discrete variables (such as gender, race/ethnicity, or education level groups).
The control variables are then 0/1 variables representing the particular groups
or their low-level interactions.
Effectively, the algorithm implements post-stratification adjustment
(\ref{eq:ps:weight}) treating each control variable as the post-stratification
variable, and cycling over them within each iteration. In terms of multivariate optimization,
this algorithm proceeds by optimizing over a each margin in sequence.
While it is very simple and very explicit in terms
of the algebra involved, it is also slow compared to Newton-Raphson-based methods.

\citet{deming:stephan:1940} stated that the algorithm minimizes the quadratic
discrepancy
$$
    \sum_{j \in \mathcal{S}} \frac{ (w_{1j}-w_{3j})^2}{w_{1j}}
$$
under the calibration constraints (\ref{eq:control:totals}).
However, the quadratic problem can be solved explicitly to
produce linear calibrated weights which lead to estimates
identical to the generalized regression (GREG) estimates
\citep[Case 1]{deville:sarndal:1992}. The raking algorithm instead solves
the optimization problem with objective function given by
\begin{equation}
    \sum_{j \in \mathcal{S}} w_{3j} \ln( w_{3j}/w_{1j}) - w_{3j} + w_{1j}
    \label{eq:raking:discrepancy}
\end{equation}
listed as Case 2 in  \citet{deville:sarndal:1992}.
Another user-contributed Stata package, \stcmd{maxentropy}
\citep{wittenberg:2010}, implements Case 4 of \citet{deville:sarndal:1992} using
Newton-Raphson optimization with analytical second derivatives,
and is much faster than \stcmd{ipfraking} described here.

\subsection{Variance estimation}
\label{subsec:variance}

Besides the primary challenge of finding a good set of weights
(which is generally solved through iterative optimization),
an additional methodological challenge with calibrated estimators
is variance estimation. If variables $x_1,\ldots,x_p$ were used
for weight calibration, then the variance of the calibrated
estimator of the survey variable $y$ is
\begin{equation}
    \mathbb{V}\bigl\{t_{wm}[y]\bigr\} = \sum_{k,l \in \mathcal{U}}
        (\pi_{kl}-\pi_k \pi_l)
        \frac{y_k - \mathbf{x}_k ' \mathbf{B}}{\pi_k}
        \frac{y_l - \mathbf{x}_l ' \mathbf{B}}{\pi_l}, m=2,3,
    \label{eq:var:calibrated}
\end{equation}
where $\mathbf{B}$ is the vector of coefficients from the
census regression,
\begin{equation}
    \mathbf{B}
    = \bigl( \sum_{i \in \mathcal{U}} \mathbf{x}_i \mathbf{x}_i' )^{-1}
      \sum_{i \in \mathcal{U}} \mathbf{x}_i y_i
    \label{eq:census:regression},
\end{equation}
This variance can be estimated with
\begin{equation}
    v\bigl\{t_{wm}[y]\bigr\} = \sum_{k,l \in \mathcal{S}}
        \frac{\pi_{kl}-\pi_k \pi_l}{pi_{kl}}
        \frac{y_k - \mathbf{x}_k ' \mathbf{b}}{\pi_k}
        \frac{y_l - \mathbf{x}_l ' \mathbf{b}}{\pi_l}, m=2,3,
    \label{eq:varest:calibrated}
\end{equation}
where the regression coefficients now solve the sample
regression problem:
\begin{equation}
    \mathbf{b}
    = \bigl( \sum_{j \in \mathcal{S}} w_j \mathbf{x}_j \mathbf{x}_j' )^{-1}
      \sum_{j \in \mathcal{S}} w_j \mathbf{x}_j y_j
    \label{eq:sample:regression},
\end{equation}
where either the probability weights $w_{1j}$ or the calibrated weights
$w_{2j}, w_{3j}$ can be used. The estimator (\ref{eq:varest:calibrated})
is difficult to use in practice, especially with the publicly released
versions of the data. First, this estimator utilizes the original design weights
$w_{1i}=\pi_i^{-1}$, so the data set must include both the calibrated weights
and the design weights, which may create confusion. Second, the end user
of the data must be given the set of the calibrating variables, which may
not be possible if confidential variables were used in calibration. Third,
this estimator is not necessarily implemented in survey packages
(a third party package \stcmd{calibest} implements (\ref{eq:varest:calibrated})
in Stata). Finally, the estimator requires second order selection probabilities,
which are rarely computed in practice. The latter is a very general issue
with the Horvitz-Thompson estimator (\ref{eq:total:sample}), as well.
Its variance is
\begin{equation}
    \mathbb{V}\bigl\{t_{w1}[y]\bigr\} = \sum_{k,l \in \mathcal{U}}
        (\pi_{kl}-\pi_k \pi_l)
        \frac{y_k}{\pi_k}
        \frac{y_l}{\pi_l}
    \label{eq:var:ht}
\end{equation}
While the second order selection probabilities are nominally required
for this estimator, in practice simplifications are taken, e.g.,
to approximate the actual design as the stratified two-stage sample
in which the primary sampling units are drawn with replacement
(as is done, for example, in \stcmd{nhanes2} \svyref{} manual example
dataset). Due to these complications, variance estimation with
calibrated data usually proceeds along the lines of
replicate variance estimation methods \citep{shao:1996,kolenikov:2010}.

When the control totals are obtained from another survey,
the sampling variability of the latter needs to be taken
into account \citep{dever:valliant:2010}. For instance,
to calibrate population surveys conducted in the USA,
American Community Survey \citep{acs:2009}
is often used for demographic variables,
and National Health Interview Survey \citep{nhis:2000}, for phone usage.
These surveys have sample sizes in hundreds of thousands,
so for typical surveys with sample sizes in hundreds to low thousands,
the impact on the standard errors is in the second or the third
decimal point, and is usually ignored.


\subsection{Pros and cons of weight calibration}

By comparing expressions (\ref{eq:var:calibrated}) and
(\ref{eq:var:ht}), we can identify the source of efficiency gains
associated with weight calibration. If the survey variable
$y$ is associated with calibration variables $x_1,\ldots,x_p$,
in the sense of having a non-trivial $R^2$ in the census regression,
then the (asymptotic) variance of the calibrated estimator is
$(1-R^2)$ times that of Horvitz-Thompson estimator.

Weight calibration also works to reduce non-response
and coverage errors \citep{kott:2006}.
With response rates on decline, reducing the potential
non-response bias is the greatest concern of the survey agencies.

Weight calibration comes with some costs, too. From analytic perspective,
manipulating the weights almost inevitably leads to increase in their
variation, which in turn leads to increases in the design effects.
For the unequal probability sample without stratification or clustering,
\citet{korn:graubard:1999} show that the design effect is
\begin{equation}
    {\rm DEFF}_w = \frac{\sum_{j \in \mathcal{S}} w_j^2 }%
       {\Bigl(\sum_{j \in \mathcal{S}} w_j\Bigr)^2}
    = 1 + {\rm CV}_w^2
    \label{eq:deff:weights}
\end{equation}
where ${\rm CV}_w$ is the coefficient of variation of weights
(a simple standard deviation divided by the simple mean).
In practice, I have encountered increases of this coefficient
of variation between 20\% and 100\% on the relative scale,
or between 0.2 and 1.5 on the absolute scale, for design effects
varying between 1 and 2 in the typical public opinion surveys.
From practical perspective, weight calibration requires
additional time expenses by statisticians preparing the data,
which increases the cost of the survey and the time elapsing
between the end of the data collection period and delivery
of the final data set. Additionally, as noted in section
\ref{subsec:variance}, variance estimation with calibrated
data tends to get complicated.

\subsection{Weight trimming}
\label{subsec:trimming}

As expression (\ref{eq:deff:weights}) shows, it is undesirable
for a survey to have a large spread of weights \citep{theberge:2000}.
Otherwise, many survey
estimates are unduly affected by the observations with large weights,
while those with small weights make but minimal contributions.
The impact of the observations with high weights will be exacerbated
in the analysis of domains, where these observations will stand apart
even more given the smaller sizes of domains.
For these reasons, weights are often {\it trimmed}: the largest
weights are reduced (say, all the weights greater than the largest allowable
number are reduced to that number), and the smallest weights are increased,
so that for all $j$, $L \le w_{3j} \le U$ for some $L$ and $U$.
Alternatively, the relative change in weights can be constrained:
for all $j$, $l \le w_{3j}/w_{1j} \le u$ for some $l$ and $u$.
Weight trimming may introduce bias, so the amount of trimming needs to be
seen as a trade-off between an apparent efficiency improvement
and latent bias \citep{elliott:2008}.


\section{Package description}

\subsection{Syntax}

\begin{stsyntax}
ipfraking
\optif\
\optin\
\optweight\
,
\underbar{ctot}al({\it matname} [{\it matname \ldots}]
\optional{
\underbar{gen}erate(\newvarname)
replace
double
\underbar{iter}ate(\num)
\underbar{tol}erance(\num)
\underbar{ctrltol}erance(\num)
trace
\underbar{nodiv}ergence
trimhirel(\num)
trimhiabs(\num)
trimlorel(\num)
trimloabs(\num)
trimfrequency(once|sometimes|often)
meta
nograph
}
\end{stsyntax}

\hangpara
Note that \stcmd{[pw=\varname]} is required, and must contain the initial weights.

\subsection{Options}

\subsubsection{Essential options}

\hangpara
\stcmd{\underbar{ctot}al(}{\it matname} \LB{\it matname \ldots}\RB\stcmd{)}
supplies the names of the matrices that contain the control
totals, as well as meta-data about the variables to be used
in calibration.

\begin{sttech}
The row and column names of the control total matrices
(see \pref{matrix rownames}) should be formatted as follows.
\begin{itemize}
    \item \stcmd{rownames}: the name of the control variable
    \item \stcmd{colnames}: the values the control variables takes
    \item \stcmd{coleq}: the name of the variable for which total is computed;
          typically it is identically equal to 1.
\end{itemize}
\end{sttech}

\hangpara
\stcmd{\underbar{gen}erate(\newvarname)}
contains the name of the new variable to contain the raked weights.

\hangpara
\stcmd{replace} indicates that the weight variable supplied in the
\stcmd{[pw=\varname]} expression should be overwritten with the new weights.

\morehang
One of \stcmd{generate()} or \stcmd{replace} must be specified.

\subsubsection{Convergence}

\begin{sttech}
Convergence in \stcmd{ipfraking} is defined in terms of the maximum relative
change in weights:
\begin{equation}
    L_k = \max_{j \in \mathcal{S}} \frac{|w_j^{k,p}-w_j^{k-1,p}|}{w_j^{k-1,p}}
    \label{eq:conv:ratio:weights}
\end{equation}
When $L_k$ is small, $L_k < \delta_L$, it means that the weights stop 
changing between iterations, i.e., the algorithm came to its steady state.
On the other hand, if $L_k > L_{k-1}$, it means that the algorithm
may start diverging, at which point it might be reasonable to terminate it.

Once the algorithm terminates, it also checks whether the control totals
are satisfied. Specifically, for each of the control total matrices
$M_1, \ldots, M_p$, the relative difference vs.\ the corresponding weighted
sample totals $\hat M_1, \ldots, \hat M_p$ is computed:
\begin{equation}
    m_c = {\rm mreldif}( \hat M_c, M_c )
    \label{eq:conv:mreldif}
\end{equation}
where the maximum relative difference of two matrices
$$
{\rm mreldif}(A,B) = \max_{ij} \frac{|a_{ij}-b_{ij}|}{1+|b_{ij}|}
$$
as defined in \dref{functions}. A control relation is satisfied
if $m_c < \delta_M$; otherwise, a warning is issued.

\end{sttech}

\hangpara
\stcmd{\underbar{tol}erance(\num)} defines the $L_k$-convergence
criterion, i.e., $\delta_L$. The default is $\delta_L = 10^{-6}$.

\hangpara
\stcmd{\underbar{iter}ate(\num)} specifies the maximum number 
of iterations $K$.
The default is \stcmd{iterate(2000)}.

\morehang
Iterations continue until either $k \ge K$, $L_k < \delta_L$ or
$L_k > L_{k-1}$.

\hangpara
\stcmd{\underbar{ctrltol}erance(\num)} defines the criterion $\delta_M$ to 
assess the accuracy of the control totals. It does not impact
iterations; it only serves as the final quality control check
after the algorithm terminates as defined above.

\hangpara
\stcmd{trace} request a trace plot to be added. See
Section \ref{subsec:example:trace}.

\hangpara
\stcmd{\underbar{nodiv}ergence} overrides the check
that $L_k > L_{k-1}$, i.e., ignores this termination condition.

\subsubsection{Trimming}

\hangpara
\stcmd{trimhirel(\num)} specifies the upper bound $u$ on the adjustment 
    factor over the baseline weight. The weights
    that exceeds the baseline times this value will be trimmed down,
    so that $w_{3j} \le u w_{1j}$ for any $j\in\mathcal{S}$.

\hangpara
\stcmd{trimhiabs(\num)} specifies the upper bound $U$ on the greatest 
    value of the raked weights.  The weights that
    exceed this value will be trimmed down, so that
    $w_{3j} \le U$ for any $j\in\mathcal{S}$.

\hangpara
\stcmd{trimlorel(\num)} specifies the lower bound $l$ on the adjustment factor 
    over the baseline weight.  The weights that are smaller than the baseline 
    times this value will be increased, so that
    $w_{3j} \ge l w_{1j}$ for any $j\in\mathcal{S}$.

\hangpara
\stcmd{trimloabs(\num)} specifies the lower bound $L$ on the smallest value 
    of the raked weights.  The weights that are smaller than this value will 
    be increased to this value, so that 
    $w_{3j} \ge L$ for any $j\in\mathcal{S}$.

\hangpara
\stcmd{trimfreqency({\it keyword})} specifies when the trimming operations 
    are to be performed. The following keywords are recognized:

\morehang \stcmd{often} means that the trimming operations will be performed 
    after each marginal adjustment, i.e., within each subiteration inside 
    Step 3 of the algorithm.

\morehang \stcmd{sometimes} means that the trimming operations will be performed 
    in the end of each iteration, i.e., after Step 3.

\morehang \stcmd{once}
means that the trimming operation will be performed after the algorithm terminates.

See Section \ref{subsec:example:trimming}.

\subsubsection{Miscellaneous}

\hangpara
\stcmd{double} specifies that the new variable named in \stcmd{generate()}
option should be generated as double type. See \dref{data types}.


\hangpara
\stcmd{meta} puts the name(s) of the control vectors and the achieved control
    accuracies $m_c$ as characteristics stored with the variable specified in
    \stcmd{generate()} option. See Section \ref{subsec:example:meta}.


\section{Examples}

\subsection{Basic syntax and input requirements}

In this very simple example, I shall demonstrate the basic mechanics of
\stcmd{ipfraking}, its input requirements and output. Do not take the numbers
very seriously, but pay attention to how the inputs look like, and what
messages may be produced by \stcmd{ipfraking}.


\begin{stexample}[Example 1]

We shall work with the standard example of \stcmd{svy} data,
an excerpt from NHANES II data set available from Stata Corp.\ website.
We shall introduce some small changes to the data so that
\stcmd{ipfraking} will have some work to do.

\begin{stlog}
\input{ipfr.example1.prep.log.tex}\nullskip
\end{stlog}

Let us now look at the matrices that will serve as an input to the
raking procedure.

\begin{stlog}
\input{ipfr.example1.list.log.tex}\nullskip
\end{stlog}

The input requirements in terms of control totals are made as simple as possible.
If a higher quality survey is available, all the survey statistician needs to do
is to obtain the totals for the categories of the control variables,
and save the name of that variable along with the matrix.
Arguably, the column equation names would be more appropriate,
but in order to minimize the manipulations to be performed on the
matrices, I chose to code \stcmd{ipfraking} to expect the name
of the variable as the row name of the matrix. Note that the \stcmd{total}
is computed with \stcmd{over( \ldots, nolabel)} suboption to suppress
the otherwise informative labeling of the categories;
\stcmd{ipfraking} expects the numeric values of the categories
as column names (see \pref{matrix rownames}).
The name of the matrix itself is immaterial, but I believe it is
a good programming practice
to have informative names \citep{mcconnell:2004}. Thus the names
of the matrices in the examples generally follow the convention
{\it data{\_}source}{\_}{\it variable}.

We are now ready to run \stcmd{ipfraking} and see what it produces.

\begin{stlog}
\input{ipfr.example1.run.log.tex}\nullskip
\end{stlog}

In this simple case with just two control variables
and the control totals that are not very different from the
existing sample totals, the procedure converged very quickly
in three iterations. A diagnostic message was produced upfront
by \stcmd{ipfraking} informing in apparent differences in
total population counts as obtained from the different
control total matrices. As a result, the control totals
for the variable that was adjusted first (\stcmd{sex}),
could not match the required control totals even after the
weights converged in the sense of differing little between
iterations. Both of these warnings are only produced when
problems are encountered.

The summary table is always produced, and shows some relevant
characteristics of the original weights $w_{1j}$, the raked weights
$w_{3j}$, and the raking ratios $w_{3j}/w_{1j}$. As expected,
the coefficient of variation went up from 0.645 to 0.672.

The graphic output produced by \stcmd{ipfraking} is shown on
Figure \ref{fig:example1}. Generally, we would want to inspect these
graphs to see if there any unusual patterns. Also, these graphs
may inform later trimming decisions: the trimming limits can be
chosen to conform to the breaks in the distributions of
the untrimmed raked weights.

\begin{figure}[h!]
\begin{center}
\epsfig{file=ipfraking_example1}
\end{center}
\caption{Histograms of the raked weights and calibration ratios, Example 1.}
\label{fig:example1}
\end{figure}

\end{stexample}

\subsection{Preparing control matrices from scratch}

In many situations, the control totals will be obtained
from outside of Stata, and need to be prepared to work
with \stcmd{ipfraking}.

\begin{stexample}[Example 2]

Suppose I wanted to calibrate
the NHANES II data set to the latest control totals available
from the US Census Bureau website. Using the tables
S0101,
from the 2011 American Community Survey 1-year estimates,
the following figures can be obtained.

\begin{tabular}{lc}
    Group & Population \\
    \hline
    \multicolumn{2}{c}{ACS 2011 1-year estimates, Table S0101} \\
    Male, total & 153,267,860 \\
    Ages 20--39 & 27.4\% \\
    Ages 40--59 & 27.5\% \\
    Ages 60+    & 17.3\% \\
    Female, total & 158,324,057 \\
    Ages 20--39 & 26.0\% \\
    Ages 40--59 & 27.6\% \\
    Ages 60+    & 20.7\% \\
    \multicolumn{2}{c}{US Census Bureau 2011 projections, Table NST-EST2011-01} \\
    Northeast & 55,521,598 \\
    Midwest   & 67,158,835 \\
    South     & 116,046,736 \\
    West      & 72,864,748 \\
    \multicolumn{2}{c}{US Census Bureau 2011 projections, Table NC-EST2011-03} \\
    White     & 243,470,497 \\
    Black     & 40,750,746 \\
    Other     & 27,370,674 \\
    \hline
    Total     & 311,591,917
\end{tabular}

Thus, we have information in the two-way age by sex table, as well
as two additional margins. We shall need an additional sex-by-age group variable,
and we shall try to make its values somewhat informative:

\begin{stlog}
\input{ipfr.example2.prep.log.tex}\nullskip
\end{stlog}

With that, the matrices will have to be defined explicitly,
and their labels need to be hand-coded, too (see \pref{matrix rownames}).
Note that the US Census Bureau 2011
projections relate to the total population, while the target population
of the study is the population age 20+. Assuming that the age structure
is the same across regions and races, the control totals for region and race
need to be rescaled to the adult population to avoid the warning messages.
(More accurate figures can be obtained from ACS microdata which can be downloaded
from the U.S.\ Census Bureau website.)

\begin{stlog}
\input{ipfr.example2.mat.log.tex}\nullskip
\end{stlog}

Let us check the matrix entries and labels once again before
producing the weights.

\begin{stlog}
\input{ipfr.example2.list.log.tex}\nullskip
\end{stlog}

As the labels appear to be in place, let us run \stcmd{ipfraking}:

\begin{stlog}
\input{ipfr.example2.run.log.tex}\nullskip
\end{stlog}

The diagnostic plots for these weights are given in Figure \ref{fig:example2}.

\begin{figure}[h!]
\begin{center}
\epsfig{file=ipfraking_example2}
\end{center}
\caption{Histograms of the raked weights and calibration ratios, Example 2.}
\label{fig:example2}
\end{figure}

\end{stexample}

\subsection{Trimming options}
\label{subsec:example:trimming}


As discussed in Section \ref{subsec:trimming} above, if variability of the weights
becomes excessive, the weights can be trimmed by restricting the extremes.
Using \stcmd{ipfraking} options, upper and/or lower limits can be defined
for either the absolute values of the weights, or the relative changes from
the base weights. The frequency of the trimming operations can also be controlled.
Trimming can be applied once to the final data (\stcmd{trimfreq(once)}), i.e.,
effectively after Step 6 of the algorithm given in Section
\ref{subsec:raking:algorithm}. Alternatively, trimming can be applied after
every iteration, i.e., effectively after Step 3 of the algorithm.
Finally, trimming can be applied after each sub-iteration within Step 3.

\begin{stexample}[Example 3]

Inspecting the histograms on Figure \ref{fig:example2}, it appears reasonable
to restrict the upper tail of the raked weights. A more detailed investigation
of the histogram reveals a somewhat greater concentration of the raked weights
around the value of 160,000, and sparse bars beyond 200,000. This latter number
will be used as the top cut-off point for trimming.

\begin{stlog}
\input{ipfr.example3.trimabs.log.tex}\nullskip
\end{stlog}

The resulting coefficient of variation of weights, 0.857, is slightly
better than that with unrestricted range of weights, 0.872. The summary also shows
that the weights were capped at 200,000, as requested.

Setting the absolute limits on the range of the raked weights is often
very subjective. A somewhat better plan might be to set limits in terms
of the range of the base weights, as shown in the next example. The relative
change in the weights is also bounded with \stcmd{trimhirel()} option.

\begin{stlog}
\input{ipfr.example3.trimsum.log.tex}\nullskip
\end{stlog}

\end{stexample}

Setting the trimming options that are too aggressive not only leads
to bias of the final estimates, but can impediment convergence, as was
seen in the last example.

\subsection{Tracking convergence}
\label{subsec:example:trace}

Let us now look in more detail into the issue of trimming frequency,
and demonstrate another diagnostic plot that can be produced by
\stcmd{ipfraking}.

\begin{stexample}[Example 4]

We return to the first set of options of Example 3, and

\begin{stlog}
\input{ipfr.example4.sometimes.log.tex}\nullskip
\end{stlog}

The option \stcmd{trace} requests that trace plots are added to
the diagnostic plots, as shown on Figure \ref{fig:example4:sometimes}.
The trace plots are presented on the absolute scale and on the log scale.
The exponentially declining discrepancy appears to be a general phenomenon.
In other words, convergence to the control totals roughly follows
the rate of $\rm{const} \times \alpha^K$ for some $\alpha<1$, where $K$ is
the number of iterations. When convergence is very slow, or the sample
size is very large, this rule may be helpful in determining the number
of iterations necessary to achieve the required accuracy, and hence
the expected computing time.

\begin{figure}[h!]
\begin{center}
\epsfig{file=ipfraking_example4_sometimes}
\end{center}
\caption{Diagnostic plots for Example 4.}
\label{fig:example4:sometimes}
\end{figure}

While \stcmd{trimfreq(sometimes)} is the default option when
other trimming options are specified, the behavior can be changed
with explicit specification of trimming frequency.

\begin{stlog}
\input{ipfr.example4.often.log.tex}\nullskip
\end{stlog}

In this example, trimming the weights after adjusting each of the margins
led to fewer iterations. This may or may not translate to lower overall
computing times as more computing is performed within each iteration.

\end{stexample}

\subsection{Metadata}
\label{subsec:example:meta}

The results of raking operations can be stored with the newly created
weight variables for later review and reproduction of the results.

\begin{stexample}[Example 5]

\begin{stlog}
\input{ipfr.example5.log.tex}\nullskip
\end{stlog}

\end{stexample}

The following characteristics are stored with the newly created weight variable
(see \pref{char}).

\begin{tabular}{ll}
    \stcmd{command} & The full command as typed by the user \\
    {\it matrix name} & The relative matrix difference from the corresponding \\
                    & control total, see \dref{functions} \\
    \stcmd{trimhiabs}, \stcmd{trimloabs}, & Corresponding trimming options,
                    if specified \\
    \stcmd{trimhirel}, \stcmd{trimlorel}, & \\
    \stcmd{trimfrequency}
\end{tabular}

Also, \stcmd{ipfraking} stores the notes regarding the control matrices
used, and which of the margins did not match the control totals, if any.
See \dref{notes}.

\subsection{Replicate weights}

As discussed in Section \ref{subsec:variance}, one of the greater challenges
of weight calibration is ensuring that variance estimates take into account
the greater precision achieved by adjusting the sample towards the fixed
population quantities. As estimating the variances using linearization
is cumbersome, replicate variance estimation provides a more attractive
alternative.

\begin{stexample}[Example 6]

The simplest code for calibrated replicate weights is obtained by calling
\stcmd{ipfraking} from within \stcmd{bsweights} \citep{kolenikov:2010}
that can pass the name of a replicate weight variable to an arbitrary
calibration routine.

\begin{stlog}
\input{ipfr.example6.bsw.log.tex}\nullskip
\end{stlog}

The options of \stcmd{bsweights} request 310 replicate weights
(a multiple of 31 strata), resample one less PSU than available in
a given stratum, and obtain the first-order within a stratum.
With the 2 PSU/stratum design and these options, \stcmd{bsweights}
produces random half-samples of data.
For explanations of these and other options,
see \citet{kolenikov:2010}. The procedure took about 8 minutes
on a laptop computer, which can be considered moderately
computationally intensive beyond interactive.
A new option of \stcmd{ipfraking} in the above code is
\stcmd{nograph} that suppresses the histograms.

A more compact set of weights can be developed based on the existing
BRR weights and a slightly more explicit code cycling over the weight
variables:

\begin{stlog}
\input{ipfr.example6.brr.log.tex}\nullskip
\end{stlog}

\end{stexample}

{\bf CAUTION:} the input weights for the replicate weight calibration
must be the probability replicate weights. The existing NHANES II weights
have been adjusted for non-response and calibrated by the data provider,
and are used above for demonstration purposes. I shall reiterate this
point in discussing the workflow of the weight development.

\subsection{Error messages and troubleshooting}

The following critical errors will stop execution of
\stcmd{ipfraking}.

\noindent
{\tt pweight is required}

\morehang
    The \stcmd{[pweight=\ldots]} component of \stcmd{ipfraking}
    syntax is required. Probability weights must be specified as
    inputs to \stcmd{ipfraking}.

\noindent
    {\tt ctotal() is required}

    \morehang
    The \stcmd{ctotal()} component of \stcmd{ipfraking}
    syntax is required. Names of the matrices containing the
    control totals must be specified.

    \noindent
    {\tt one and only one of generate() or replace must be specified}

    \morehang
    Either \stcmd{generate()} option with the name of the new variable
    must be supplied to \stcmd{ipfraking}, or \stcmd{replace} to replace
    the variable specified in \stcmd{[pw=\ldots]} statement.

    \noindent
    {\tt raking procedure appears diverging}

    \morehang
    The maximum relative difference of weights has increased since last
    iteration. This may or may not indicate a problem. Re-run \stcmd{ipfraking}
    with \stcmd{nodivergence} option to override the warning.

    \noindent
    {\tt cannot process matrix {\it matrix{\_}name}}

    \morehang
    For whatever reason, \stcmd{ipfraking} could not process this matrix.
    The matrix may not have been defined, or the variables in this matrix
    cannot be found.

    \noindent
    {\tt variable {\it varname} corresponding to the control matrix
    {\it matrix{\_}name} not found}

    \morehang
    The variables contained in row or column names of this matrix
    cannot be found.

    \noindent
    {\tt {\it varname1} and {\it varname2} variables are not compatible}

    \morehang
    When running \stcmd{total} {\it varname1}\stcmd{, over(}{\it varname2}\stcmd{)},
    an error was encountered. One of the variables may be a string variable
    or have missing values resulting in empty estimation sample.

    \noindent
    {\tt categories of {\it varname} do not match in the control {\it matrix{\_}name}
    and in the data (nolab option)}

    \morehang
    There was a mismatch in the categories of {\it varname} found in the data
    and in the control matrix {\it matrix{\_}name}. This could happen for any of the
    following reasons: (i) there were more categories in one than in the other;
    (ii) the entries are in the wrong order in the control matrix; (iii) the labels
    in the control matrix do not correspond to the category values in the data set;
    (iv) the control matrix was obtained via \stcmd{total}
    {\it varname2}\stcmd{, over(}{\it varname}\stcmd{)}, but \stcmd{nolabel} suboption
    of \stcmd{over()} was omitted, and the labels of the control matrix are
    ``too nice''. Tabulate {\it varname} without labels, and compare the results
    to the matrix listing of the {\it matrix{\_}name}.

    \noindent
    {\tt cannot compute controls for {\it matrix{\_}name} over
    {\it varname} with the current weights}

    \morehang
    This is a generic error message that something bad happened while
    \stcmd{ipfraking} was computing the totals for the current set of weights.
    This error message should be very rare.

The following warning messages may be produced by
\stcmd{ipfraking}. It will continue running, but you must
double-check the results for potential problems.

\noindent
    {\tt the totals of the control matrices are different}

    \morehang
    The sum of values of the control matrices are different.
    These sums will be listed for review. Convergence is still
    possible, but some of the control total checks are bound to fail.

    \noindent
    {\tt trimfrequency() option is specified without numeric settings; will be ignored}

    \morehang
    The option \stcmd{trimfrequency()} was specified without any other trimming options.
    There is no way to interpret this, and \stcmd{ipfraking} will proceed without
    trimming.

    \noindent
    {\tt trimfrequency() option is specified incorrectly, assume default value (sometimes)}

    \morehang
    As explained

    \noindent
    {\tt raking procedure appears diverging}

    \morehang
    The maximum relative difference of weights has increased since last
    iteration. This may or may not indicate a problem. Re-run \stcmd{ipfraking}
    with \stcmd{nodivergence} option to override the warning.

    \noindent
    {\tt raking procedure did not converge}

    \morehang
    The maximum number of iterations was reached, but weights continued
    changing. The user may want to increase the number of iterations
    or relax convergence criteria.

    \noindent
    {\tt the controls {\it matrix{\_}name} did not match}

    \morehang
    After convergence of weights was declared, \stcmd{ipfraking}
    checked again the control totals, and found that the results 
    differed from the target for one or more of the control total
    matrices. Any of the following can cause this: (i) the sum of 
    entries of this particular matrix differs from the others;
    (ii) the trimming options are too demanding, and do not allow
    the weights to adjust enough; (iii) the problem may not have a
    solution due to incompatible control totals or a bad sample.

    \noindent
    {\tt division by zero weighted total encountered with 
    {\it matrix{\_}name} control}

    \morehang
    The weights for a category of the control variable summed up
    to zero. \stcmd{ipfraking} will skip calibration over this
    variable and proceed to the next one.

    \noindent
    {\tt \# missing values of {\it varname} encountered; convergence will be impaired}

    \morehang
    A control variable has missing values in the calibration sample. 
    There is little way for \stcmd{ipfraking} to figure out how to deal
    with the weights for the observations with missing values. The best course
    of action is to impute them and run \stcmd{ipfraking} again.

% \section{Workflow}



\bibliographystyle{sj}
% \bibliography{everything}
\bibliography{ipfraking}

\begin{aboutauthor}
  Stanislav (Stas) Kolenikov is a Senior Survey Statistician at Abt SRBI.
  His research interests include
  applications of statistical methods in public opinion research,
  such as advanced sampling techniques, survey weighting,
  calibration, missing data imputation, and variance estimation.
  Stas has extensive experience developing and applying
  statistical methods in social sciences, with focus on structural equation
  modeling and microeconometrics. He has been writing Stata programs since
  1998 when Stata was version 5.
\end{aboutauthor}
